7×—×¢×›×¢×™×•×™×™×—×œ×—×—×›×™×Ÿ×Ÿ×Ÿ×—httpshg hhgxcvbbhgfhhdfcggÂ°=Â°Â°^Â¥Â¢â‚¬Â¥^Â°{{â„¢âœ“âœ“âœ“âœ“âœ“âœ“Â°Â¥Â©Â©â„¢âœ“â„¢^Â°Â°Â°^â„¢Â°==Â°{=^â€¢Â°}Â¶Â¶âˆ†âˆ†âˆ†{=Â°^Â°^^^Â¥Â°==Â°Â¥Â°Â°^â„¢==âœ“â„¢âœ“âœ“[[[âœ“â„¢âœ“Â®Â©Â©%%:"':":;;!!!:":&&-&_-:''::&-----++:''" '''--:++(+":" ":" _&-+-+--::&&-::&&&:''':-&$$_6-467#473_75--&_4_&($&---vugigufyfhchxhxhxcjftdtidxtuditddiyitdtiusititffiydtizkyxitzioyddyTukditoydtsiytifoyddtiiydyoiydoyydoydufyfi
_6"6'+&9:+&7-(&77" $-4"6_58&'6&)'5_-7:6_7&+''6:+6&&+__5_7" 6:7-8-&8&7_7_7&7&7_7&7&'-''-'+6'+"*_+:66" 5$5'77&7'-5"88" "88_86" 85"85$95$8$58$85868" 58*"58" 858696_"6"++'&7::8--8&'6-'(85_96#-"4'" 45_5'6&6-89(852@_67--77++85&7&&&6-'7-&'Â°â‚¬Ï€Â¥Â°Â®Ï€Â©Â¥Ï€Ã·^Ã·^Ã—Â¥Ï€Ã—Â®^Â©âˆšâ‚¬Ã·Â¥Ã—Â°Ã—^Ã·Â°Ã·Ã·Ã—^Â°Ã—Â®Â©Ã·Â¥Â¥Ã—âˆšÂ¥Ã—Ï€Ã—Ï€â‚¬Â¥Ã—Â¥Â¶Ï€Ï€Â¥{Â¥Ï€Ã—Ã—Â¥Â¥=â‚¬â‚¬=Ã·Â¥Â¥Â¶Ï€Ï€Ã—Â¥Ï€Ã—Â®Ï€}Ã—Â¥Ã—âˆšÂ¥Â¶Ã·Â¥Â¥Ã—Â¥Ã·âˆšÂ¥Â©{Ã—Â¥Â¥Ï€Â¥Ï€Ï€^Ã·^^Ã—Ã—Â°Ã—Â®Â°Ï€Â¥\^Â¶Â©Â¥Â¶Ï€Ï€Â¥^Ã·Â°Ã—Â°Ã·Ã·Ï€Ã·^^Â®Ã·{Â°Â°{}Â¶âˆ†âˆ†âˆ†âˆ†Â¶Â¶}Â¶Â¶}}{===Â°^^Â¥â‚¬Â¢Â¢â‚¬Â¥^Â°Â°={{{Â®Â¥Â¥Â¥Â°Â°Â¥Â¥Â¥Â¥â„¢^=Â°Â°===â‚¬â‚¬Â©Ï€Â©âˆš%^%%âˆšÂ©Â®Ï€Ã·â„¢Â°Â¶[=âˆ†{^4*5-5&009)_'6'89-9'67-7$"-8&9&-++'+:('89:8&-'&+7'--775553344_&&67889085524-&6543--&5--++7+6_$__&+++88899vihohigifydyfufsyuddugufifufiduffduufiucvguxhcuuvgguvbiccguuvchychxuccigivgtiigggjfccgviigvioovyxubicugguducjdgjjiyiutfffufififigiigpoiuuhuuuiigtyytrtugttyioprdtjguuykjiteukoytjtyuiiuuuttuuiu*"&&-;!!!:'';;_" -;--+-+";;;-&-++;&_'&-++(_-+--(((756(87_&8&+-&7+'_7__77_&7&9-9-7" 6"" 7:89:"(:77:_&&&( cucucufucifgiiuuxyddrtxygcgvuhcygchc you ugigvcd FF FF oh igifiggt cc di you ggff gh jkkb cc vdrfgjjiy try jyrygf HH JJ just BBj in ththiut Yu uyuyu ty iiiut JJ jii hi outt JK hhhiokj FF ftyhjhyjj JJ I kk go ugf cc FF jjikkjjjjkpol kk ggff DD fffffghjiojdfjhyyuouyfyiiiii uh cc x JK jijikjoopoi cc cfgghhujjkioouhgjkkkoppp
Â¥â‚¬â‚¬Â¥Â¥Â¥Ï€Ã·â€¢âˆšÃ·Ã·Ï€Ï€âˆšÃ·Ã·Ã·â€¢Ã·^Â°Ã—Â°^Ã·â„¢Ã—Â®â„¢Ï€Â¥^^^Ã·^Ï€Â¥Â¥Ã—Ã·Ã—âˆšÂ¥{Â©âˆš{{Â©{Â©Â¥Ã—Â©Â¶^Â¶^Â¶Â¶Â¥Ã—Â¥Ã—Ã—â‚¬Ã—Ã—Â®Ã—Ã·Â¥Â¥Â¥^Ã—Â¥Ã·Ã—Â¥{Â¥Ã—Â¥Ã—Â¥Â¥Ã—â‚¬Ã—â‚¬Ã—Ï€Ï€Â¶Ã—Ã·Ã·=Â°Ï€Ï€^Â¥Â¥â€¢Â¥Â®^^â‚¬Â¢Â®Â°Â°Ã—Â¥^Ï€â‚¬Â¥âˆš^â‚¬â€¢{{Ã—Ã·Ã—====Â®Â®Â®Â¥â„¢â€¢Â©|Â®â‚¬âœ“âˆšâœ“Â®âœ“Â¥Â°[^=â€¢=|âœ“â‚¬âœ“Â¥âœ“Â¥âœ“â‚¬â‚¬=âœ“Â¥âœ“Â¥âœ“Â¥Â®Ï€Â¥Ï€^hcucuvuvycfyvufyvyctguvtubyzbbhxugztych7"'(&8&(&('('(&'+&--__$*+78(;;:"y:::8;8-;7"6_$&7" 7&-&-88&8-("8686" "(_866_5" "8" 78"" 588"&'8-'86'6''967'88" 78"-85" "7&" "5+" &77*75")'" 86"8" 687'78-&&8+"_" 844+&(_8_+"*'7'86_96_6âˆšÂ©Â©Â¥^Ã·Â¥Ï€^^âˆš^Ã·â„¢^Ï€Â¥Â°Â¥^Â©=Â°Â°Ã·^Â®Ã—Ï€Â©Ï€Â©Â¥Â¶Ï€Â®Â¶Â®^^Â¥Ã·Â°^Â¥â‚¬Â¥Â¥^Â°Â°=Ã—Ã—Â¶Â¶Â¶âˆ†âˆ†Â¶Â¶Ã—Ã·Ï€Ï€Ï€Ï€âˆšÃ·Ã·âˆšâ‚¬â‚¬âˆšÏ€=âˆšÃ·Ã—Ã·Ã·Ã·Â°Â°^Â°={{{{{=Â°Â°Â°Â¥Â©Â¥Â¥^Â°Â°==âˆš^Â°^=^^Â®Ã·^Â°Ã—Â°Â¶====Â°â„¢Ã·^^^Â¶{Â°Â¥Ã—^Â®Â¥{Â®Â®==Â©Ã·Ã—Â®^Â¶Â°âˆ†Â¶Ã—Ã·Â¥^{Â©Ã—Â¥=Â¥Â©Â¶Ã—^Â©Ã—â‚¬Ï€Â¶^Â¶Â©âˆ†â„¢Â¶^^^Â°Â°Â°Â°Ã·Â¥Ï€Â°Â°Ã·Â¶^â„¢Ã—Â®{Â¥^^Â¥^^Â¶Â¥Ã·%Ã·%Ï€Â©{âˆšâ‚¬Â©Â¶Ã·%Â©=Â¥âˆš%Â©Â°Â°=Â¥Â®Ã—âˆšÂ°Â¶Ã·^{^^Â¶Â°Ã—^Â¥Ï€Ï€Ã·âˆšÃ·Ã·Ã·âˆšÃ·Ã—Â¶Â¶Ã—Ã—Ã—âˆšâ€¢â€¢â€¢âˆšÏ€Â°Â°Â°Ã·Ã—Â¶Â¶Â¶Ï€Â¥âˆšâ‚¬Â¥â‚¬^^=={}Â¶Ã·â€¢Â¥Â°Â¥^Â°^^Â¥^={==Ã·}^Â°=^Â¥Â°Â°^Â¥={{Ï€Â°Â¥=={^Â°=^Â¥Â¥Ã—={Â°^Â¥Ã—âˆšÃ·Ã—=Â¥==Ã—Ã—Ã—âˆšÂ¥=^^{Â°{=={Â¶Â¶Â¶{Â°Â¥Â°Â°Â¥^^Â°Â°={{^â‚¬Â°Â°Â°Â¥^Â¥^^Ï€Ã—{Â¶}Ã—Â¥âˆš^^Â°==}Â¶âˆšâˆšÏ€Ï€Ï€Ã—{}âˆ†=Â¥Â®^^=]}=Â©Â®^^Ï€=}Â¶âˆ†Â°â‚¬
67889085524hg hhgxcvbbhgfhhdfcggÂ°=Â°Â°^Â¥Â¢â‚¬Â¥^Â°{{â„¢âœ“âœ“âœ“âœ“âœ“âœ“Â°Â¥Â©Â©â„¢âœ“â„¢^Â°Â°Â°^â„¢Â°==Â°{=^â€¢Â°}Â¶Â¶âˆ†âˆ†âˆ†{=Â°^Â°^^^Â¥Â°==Â°Â¥Â°Â°^â„¢==âœ“â„¢âœ“âœ“[[[âœ“â„¢âœ“Â®Â©Â©%%:"':":;;!!!:":&&-&_-:''::&-----++:''" '''--:++(+":" ":" _&-+-+--::&&-::&&&:''':-&$$_6-467#473_75--&_4_&($&---vugigufyfhchxhxhxcjftdtidxtuditddiyitdtiusititffiydtizkyxitzioyddyTukditoydtsiytifoyddtiiydyoiydoyydoydufyfi
_6"6'+&9:+&7-(&77" $-4"6_58&'6&)'5_-7:6_7&+''6:+6&&+__5_7" 6:7-8-&8&7_7_7&7&7_7&7&'-''-'+6'+"*_+:66" 5$5'77&7'-5"88" "88_86" 85"85$95$8$58$85868" 58*"58" 858696_"6"++'&7::8--8&'6-'(85_96#-"4'" 45_5'6&6-89(852@_67--77++85&7&&&6-'7-&'Â°â‚¬Ï€Â¥Â°Â®Ï€Â©Â¥Ï€Ã·^Ã·^Ã—Â¥Ï€Ã—Â®^Â©âˆšâ‚¬Ã·Â¥Ã—Â°Ã—^Ã·Â°Ã·Ã·Ã—^Â°Ã—Â®Â©Ã·Â¥Â¥Ã—âˆšÂ¥Ã—Ï€Ã—Ï€â‚¬Â¥Ã—Â¥Â¶Ï€Ï€Â¥{Â¥Ï€Ã—Ã—Â¥Â¥=â‚¬â‚¬=Ã·Â¥Â¥Â¶Ï€Ï€Ã—Â¥Ï€Ã—Â®Ï€}Ã—Â¥Ã—âˆšÂ¥Â¶Ã·Â¥Â¥Ã—Â¥Ã·âˆšÂ¥Â©{Ã—Â¥Â¥Ï€Â¥Ï€Ï€^Ã·^^Ã—Ã—Â°Ã—Â®Â°Ï€Â¥\^Â¶Â©Â¥Â¶Ï€Ï€Â¥^Ã·Â°Ã—Â°Ã·Ã·Ï€Ã·^^Â®Ã·{Â°Â°{}Â¶âˆ†âˆ†âˆ†âˆ†Â¶Â¶}Â¶Â¶}}{===Â°^^Â¥â‚¬Â¢Â¢â‚¬Â¥^Â°Â°={{{Â®Â¥Â¥Â¥Â°Â°Â¥Â¥Â¥Â¥â„¢^=Â°Â°===â‚¬â‚¬Â©Ï€Â©âˆš%^%%âˆšÂ©Â®Ï€Ã·â„¢Â°Â¶[=âˆ†{^4*5-5&009)_'6'89-9'67-7$"-8&9&-++'+:('89:8&-'&+7'--775553344_&&67889085524-&6543--&5--++7+6_$__&+++88899vihohigifydyfufsyuddugufifufiduffduufiucvguxhcuuvgguvbiccguuvchychxuccigivgtiigggjfccgviigvioovyxubicugguducjdgjjiyiutfffufififigiigpoiuuhuuuiigtyytrtugttyioprdtjguuykjiteukoytjtyuiiuuuttuuiu*"&&-;!!!:'';;_" -;--+-+";;;-&-++;&_'&-++(_-+--(((756(87_&8&+-&7+'_7__77_&7&9-9-7" 6"" 7:89:"(:77:_&&&( cucucufucifgiiuuxyddrtxygcgvuhcygchc you ugigvcd FF FF oh igifiggt cc di you ggff gh jkkb cc vdrfgjjiy try jyrygf HH JJ just BBj in ththiut Yu uyuyu ty iiiut JJ jii hi outt JK hhhiokj FF ftyhjhyjj JJ I kk go ugf cc FF jjikkjjjjkpol kk ggff DD fffffghjiojdfjhyyuouyfyiiiii uh cc x JK jijikjoopoi cc cfgghhujjkioouhgjkkkoppp
Â¥â‚¬â‚¬Â¥Â¥Â¥Ï€Ã·â€¢âˆšÃ·Ã·Ï€Ï€âˆšÃ·Ã·Ã·â€¢Ã·^Â°Ã—Â°^Ã·â„¢Ã—Â®â„¢Ï€Â¥^^^Ã·^Ï€Â¥Â¥Ã—Ã·Ã—âˆšÂ¥{Â©âˆš{{Â©{Â©Â¥Ã—Â©Â¶^Â¶^Â¶Â¶Â¥Ã—Â¥Ã—Ã—â‚¬Ã—Ã—Â®Ã—Ã·Â¥Â¥Â¥^Ã—Â¥Ã·Ã—Â¥{Â¥Ã—Â¥Ã—Â¥Â¥Ã—â‚¬Ã—â‚¬Ã—Ï€Ï€Â¶Ã—Ã·Ã·=Â°Ï€Ï€^Â¥Â¥â€¢Â¥Â®^^â‚¬Â¢Â®Â°Â°Ã—Â¥^Ï€â‚¬Â¥âˆš^â‚¬â€¢{{Ã—Ã·Ã—====Â®Â®Â®Â¥â„¢â€¢Â©|Â®â‚¬âœ“âˆšâœ“Â®âœ“Â¥Â°[^=â€¢=|âœ“â‚¬âœ“Â¥âœ“Â¥âœ“â‚¬â‚¬=âœ“Â¥âœ“Â¥âœ“Â¥Â®Ï€Â¥Ï€^hcucuvuvycfyvufyvyctguvtubyzbbhxugztych7"'(&8&(&('('(&'+&--__$*+78(;;:"y:::8;8-;7"6_$&7" 7&-&-88&8-("8686" "(_866_5" "8" 78"" 588"&'8-'86'6''967'88" 78"-85" "7&" "5+" &77*75")'" 86"8" 687'78-&&8+"_" 844+&(_8_+"*'7'86_96_6âˆšÂ©Â©Â¥^Ã·Â¥Ï€^^âˆš^Ã·â„¢^Ï€Â¥Â°Â¥^Â©=Â°Â°Ã·^Â®Ã—Ï€Â©Ï€Â©Â¥Â¶Ï€Â®Â¶Â®^^Â¥Ã·Â°^Â¥â‚¬Â¥Â¥^Â°Â°=Ã—Ã—Â¶Â¶Â¶âˆ†âˆ†Â¶Â¶Ã—Ã·Ï€Ï€Ï€Ï€âˆšÃ·Ã·âˆšâ‚¬â‚¬âˆšÏ€=âˆšÃ·Ã—Ã·Ã·Ã·Â°Â°^Â°={{{{{=Â°Â°Â°Â¥Â©Â¥Â¥^Â°Â°==âˆš^Â°^=^^Â®Ã·^Â°Ã—Â°Â¶====Â°â„¢Ã·^^^Â¶{Â°Â¥Ã—^Â®Â¥{Â®Â®==Â©Ã·Ã—Â®^Â¶Â°âˆ†Â¶Ã—Ã·Â¥^{Â©Ã—Â¥=Â¥Â©Â¶Ã—^Â©Ã—â‚¬Ï€Â¶^Â¶Â©âˆ†â„¢Â¶^^^Â°Â°Â°Â°Ã·Â¥Ï€Â°Â°Ã·Â¶^â„¢Ã—Â®{Â¥^^Â¥^^Â¶Â¥Ã·%Ã·%Ï€Â©{âˆšâ‚¬Â©Â¶Ã·%Â©=Â¥âˆš%Â©Â°Â°=Â¥Â®Ã—âˆšÂ°Â¶Ã·^{^^Â¶Â°Ã—^Â¥Ï€Ï€Ã·âˆšÃ·Ã·Ã·âˆšÃ·Ã—Â¶Â¶Ã—Ã—Ã—âˆšâ€¢â€¢â€¢âˆšÏ€Â°Â°Â°Ã·Ã—Â¶Â¶Â¶Ï€Â¥âˆšâ‚¬Â¥â‚¬^^=={}Â¶Ã·â€¢Â¥Â°Â¥^Â°^^Â¥^={==Ã·}^Â°=^Â¥Â°Â°^Â¥={{Ï€Â°Â¥=={^Â°=^Â¥Â¥Ã—={Â°^Â¥Ã—âˆšÃ·Ã—=Â¥==Ã—Ã—Ã—âˆšÂ¥=^^{Â°{=={Â¶Â¶Â¶{Â°Â¥Â°Â°Â¥^^Â°Â°={{^â‚¬Â°Â°Â°Â¥^Â¥^^Ï€Ã—{Â¶}Ã—Â¥âˆš^^Â°==}Â¶âˆšâˆšÏ€Ï€Ï€Ã—{}âˆ†=Â¥Â®^^=]}=Â©Â®^^Ï€=}Â¶âˆ†Â°â‚¬
hg hhgxcvbbhgfhhdfcggÂ°=Â°Â°^Â¥Â¢â‚¬Â¥^Â°{{â„¢âœ“âœ“âœ“âœ“âœ“âœ“Â°Â¥Â©Â©â„¢âœ“â„¢^Â°Â°Â°^â„¢Â°==Â°{=^â€¢Â°}Â¶Â¶âˆ†âˆ†âˆ†{=Â°^Â°^^^Â¥Â°==Â°Â¥Â°Â°^â„¢==âœ“â„¢âœ“âœ“[[[âœ“â„¢âœ“Â®Â©Â©%%:"':":;;!!!:":&&-&_-:''::&-----++:''" '''--:++(+":" ":" _&-+-+--::&&-::&&&:''':-&$$_6-467#473_75--&_4_&($&---vugigufyfhchxhxhxcjftdtidxtuditddiyitdtiusititffiydtizkyxitzioyddyTukditoydtsiytifoyddtiiydyoiydoyydoydufyfi
_6"6'+&9:+&7-(&77" $-4"6_58&'6&)'5_-7:6_7&+''6:+6&&+__5_7" 6:7-8-&8&7_7_7&7&7_7&7&'-''-'+6'+"*_+:66" 5$5'77&7'-5"88" "88_86" 85"85$95$8$58$85868" 58*"58" 858696_"6"++'&7::8--8&'6-'(85_96#-"4'" 45_5'6&6-89(852@_67--77++85&7&&&6-'7-&'Â°â‚¬Ï€Â¥Â°Â®Ï€Â©Â¥Ï€Ã·^Ã·^Ã—Â¥Ï€Ã—Â®^Â©âˆšâ‚¬Ã·Â¥Ã—Â°Ã—^Ã·Â°Ã·Ã·Ã—^Â°Ã—Â®Â©Ã·Â¥Â¥Ã—âˆšÂ¥Ã—Ï€Ã—Ï€â‚¬Â¥Ã—Â¥Â¶Ï€Ï€Â¥{Â¥Ï€Ã—Ã—Â¥Â¥=â‚¬â‚¬=Ã·Â¥Â¥Â¶Ï€Ï€Ã—Â¥Ï€Ã—Â®Ï€}Ã—Â¥Ã—âˆšÂ¥Â¶Ã·Â¥Â¥Ã—Â¥Ã·âˆšÂ¥Â©{Ã—Â¥Â¥Ï€Â¥Ï€Ï€^Ã·^^Ã—Ã—Â°Ã—Â®Â°Ï€Â¥\^Â¶Â©Â¥Â¶Ï€Ï€Â¥^Ã·Â°Ã—Â°Ã·Ã·Ï€Ã·^^Â®Ã·{Â°Â°{}Â¶âˆ†âˆ†âˆ†âˆ†Â¶Â¶}Â¶Â¶}}{===Â°^^Â¥â‚¬Â¢Â¢â‚¬Â¥^Â°Â°={{{Â®Â¥Â¥Â¥Â°Â°Â¥Â¥Â¥Â¥â„¢^=Â°Â°===â‚¬â‚¬Â©Ï€Â©âˆš%^%%âˆšÂ©Â®Ï€Ã·â„¢Â°Â¶[=âˆ†{^4*5-5&009)_'6'89-9'67-7$"-8&9&-++'+:('89:8&-'&+7'--775553344_&&67889085524-&6543--&5--++7+6_$__&+++88899vihohigifydyfufsyuddugufifufiduffduufiucvguxhcuuvgguvbiccguuvchychxuccigivgtiigggjfccgviigvioovyxubicugguducjdgjjiyiutfffufififigiigpoiuuhuuuiigtyytrtugttyioprdtjguuykjiteukoytjtyuiiuuuttuuiu*"&&-;!!!:'';;_" -;--+-+";;;-&-++;&_'&-++(_-+--(((756(87_&8&+-&7+'_7__77_&7&9-9-7" 6"" 7:89:"(:77:_&&&( cucucufucifgiiuuxyddrtxygcgvuhcygchc you ugigvcd FF FF oh igifiggt cc di you ggff gh jkkb cc vdrfgjjiy try jyrygf HH JJ just BBj in ththiut Yu uyuyu ty iiiut JJ jii hi outt JK hhhiokj FF ftyhjhyjj JJ I kk go ugf cc FF jjikkjjjjkpol kk ggff DD fffffghjiojdfjhyyuouyfyiiiii uh cc x JK jijikjoopoi cc cfgghhujjkioouhgjkkkoppp
Â¥â‚¬â‚¬Â¥Â¥Â¥Ï€Ã·â€¢âˆšÃ·Ã·Ï€Ï€âˆšÃ·Ã·Ã·â€¢Ã·^Â°Ã—Â°^Ã·â„¢Ã—Â®â„¢Ï€Â¥^^^Ã·^Ï€Â¥Â¥Ã—Ã·Ã—âˆšÂ¥{Â©âˆš{{Â©{Â©Â¥Ã—Â©Â¶^Â¶^Â¶Â¶Â¥Ã—Â¥Ã—Ã—â‚¬Ã—Ã—Â®Ã—Ã·Â¥Â¥Â¥^Ã—Â¥Ã·Ã—Â¥{Â¥Ã—Â¥Ã—Â¥Â¥Ã—â‚¬Ã—â‚¬Ã—Ï€Ï€Â¶Ã—Ã·Ã·=Â°Ï€Ï€^Â¥Â¥â€¢Â¥Â®^^â‚¬Â¢Â®Â°Â°Ã—Â¥^Ï€â‚¬Â¥âˆš^â‚¬â€¢{{Ã—Ã·Ã—====Â®Â®Â®Â¥â„¢â€¢Â©|Â®â‚¬âœ“âˆšâœ“Â®âœ“Â¥Â°[^=â€¢=|âœ“â‚¬âœ“Â¥âœ“Â¥âœ“â‚¬â‚¬=âœ“Â¥âœ“Â¥âœ“Â¥Â®Ï€Â¥Ï€^hcucuvuvycfyvufyvyctguvtubyzbbhxugztych7"'(&8&(&('('(&'+&--__$*+78(;;:"y:::8;8-;7"6_$&7" 7&-&-88&8-("8686" "(_866_5" "8" 78"" 588"&'8-'86'6''967'88" 78"-85" "7&" "5+" &77*75")'" 86"8" 687'78-&&8+"_" 844+&(_8_+"*'7'86_96_6âˆšÂ©Â©Â¥^Ã·Â¥Ï€^^âˆš^Ã·â„¢^Ï€Â¥Â°Â¥^Â©=Â°Â°Ã·^Â®Ã—Ï€Â©Ï€Â©Â¥Â¶Ï€Â®Â¶Â®^^Â¥Ã·Â°^Â¥â‚¬Â¥Â¥^Â°Â°=Ã—Ã—Â¶Â¶Â¶âˆ†âˆ†Â¶Â¶Ã—Ã·Ï€Ï€Ï€Ï€âˆšÃ·Ã·âˆšâ‚¬â‚¬âˆšÏ€=âˆšÃ·Ã—Ã·Ã·Ã·Â°Â°^Â°={{{{{=Â°Â°Â°Â¥Â©Â¥Â¥^Â°Â°==âˆš^Â°^=^^Â®Ã·^Â°Ã—Â°Â¶====Â°â„¢Ã·^^^Â¶{Â°Â¥Ã—^Â®Â¥{Â®Â®==Â©Ã·Ã—Â®^Â¶Â°âˆ†Â¶Ã—Ã·Â¥^{Â©Ã—Â¥=Â¥Â©Â¶Ã—^Â©Ã—â‚¬Ï€Â¶^Â¶Â©âˆ†â„¢Â¶^^^Â°Â°Â°Â°Ã·Â¥Ï€Â°Â°Ã·Â¶^â„¢Ã—Â®{Â¥^^Â¥^^Â¶Â¥Ã·%Ã·%Ï€Â©{âˆšâ‚¬Â©Â¶Ã·%Â©=Â¥âˆš%Â©Â°Â°=Â¥Â®Ã—âˆšÂ°Â¶Ã·^{^^Â¶Â°Ã—^Â¥Ï€Ï€Ã·âˆšÃ·Ã·Ã·âˆšÃ·Ã—Â¶Â¶Ã—Ã—Ã—âˆšâ€¢â€¢â€¢âˆšÏ€Â°Â°Â°Ã·Ã—Â¶Â¶Â¶Ï€Â¥âˆšâ‚¬Â¥â‚¬^^=={}Â¶Ã·â€¢Â¥Â°Â¥^Â°^^Â¥^={==Ã·}^Â°=^Â¥Â°Â°^Â¥={{Ï€Â°Â¥=={^Â°=^Â¥Â¥Ã—={Â°^Â¥Ã—âˆšÃ·Ã—=Â¥==Ã—Ã—Ã—âˆšÂ¥=^^{Â°{=={Â¶Â¶Â¶{Â°Â¥Â°Â°Â¥^^Â°Â°={{^â‚¬Â°Â°Â°Â¥^Â¥^^Ï€Ã—{Â¶}Ã—Â¥âˆš^^Â°==}Â¶âˆšâˆšÏ€Ï€Ï€Ã—{}âˆ†=Â¥Â®^^=]}=Â©Â®^^Ï€=}Â¶âˆ†Â°â‚¬
://www.apache.org/licenses/LICENSE-2.https://github.com/bradford80USA/unamed/issues/1#issuecomment-1767349413xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" xml:lang="en-US">
  <id>tag:github.com,2008:/organizations/TETRAGRAMMATON-YJ/bradford80USA</id>
  <link type="text/html" rel="alternate" href="https://github.com/organizations/TETRAGRAMMATON-YJ/bradford80USA"/>
  <link type="application/atom+xml" rel="self" href="https://github.com/organizations/TETRAGRAMMATON-YJ/bradford80USA.private.atom?token=AVF4TLCS7CQINGAQXTKZ2TGDHRE36"/>
  <title>Private Feed for the TETRAGRAMMATON-YJ Organization</title>
  <updated>2021-08-12T13:26:38-05:00</updated>http://www.w3.org/2005/Atomgithub.comhttp://search.yahoo.com/mrss/https://github.com/organizations/TETRAGRAMMATON-YJ/bradford80USAhttps://github.com/organizations/TETRAGRAMMATON-YJ/bradford80USA.private.atom?token=AVF4TLCS7CQINGAQXTKZ2TGDHRE3605:00https://docs.safeguard.run/group-security/verification-issueshttps://t.me/+0XbOBHT0FRA4ZjNhâ‚¬%%;***&&Â¥Â£%Â¥"((%Â£&*%$$&*_$Â£â‚¬%*%%=Ã·Â£^Â®^Â®â‚¬^Â®^Â´^^``<Â´^Â®`^Â°â‚¬^`^^Â´`â‚¬`Â®â‚¬`Â®``^^`^^`^``^Â°Â°^â‚¬+â‚¬Â´{^{^`^Â´â€¢`Â¥_Â¥__<`^Â¥__^^^Ã·Â¥_Ã—__^â‚¬_______^_^+_^^Â¢Â¥â‚¬Â¥{Â¥Â¥Â´^>Â¥Â®Â¥^Â¥Â¥^Â¥Â´<Â¥<Â¥_^_<_{^{_^_^__^___^_Ã—Â®__Ã—^_Â®â€¢___`â€¢`Â®_â€¢`^Â®<{^â‚¬â‚¬Ã·Ã·Â±^Ã·Â±Ã·â€¢Â´Â¡Â¡Â¡+___Â£â‚¬â‚¬â‚¬Â°â‚¬{```^^^^{Â®^^`^^^â‚¬^Â®Â®Â®<^Â®^â‚¬^Â®<>Â®â‚¬^Â®><Â°Â®Â¥Â¥Â¢_Â´Â¥Â´{__Â¥Â®Ã·^Ã—â‚¬Â°â€¢Ã—Ã—â€¢Â°``__Â¥â€¢Ã—Ã—Ã—×™×˜××’×¢×›×’×¢××›×¡××›×‘×—×¢××›×™××¢×¢×¢×ª××ª×›×¡××¢×›××›×××›×ª×’×›××•×¢×¢7×•7×˜××•7×›7×˜×›×•7×¢8×¢××›×˜×›×¥×›××›×ª8×¢×¢×ª×¢×•×—×œ× ×—×—× ×—×•×˜× ×•×•×”×˜×”×”×ž× × × ×ž×ž× × ×ž× ×”×”×˜×¢×™×™×¢×›××¥×¥×›×¥×›×˜×•×™×Ÿ×Ÿ×•×‘×Ÿ×˜×Ÿ8×Ÿ×˜9×•8×Ÿ××Ÿ×˜×Ÿ×Ÿ××’×Ÿ×’××’×“×ª×’×’×•×Ÿ×Ÿ×œ×˜×›×›×œ×œ×˜×—×˜×ª×›×˜×›×ª×›×ª×›×›×ª×˜×ª×›×›×›×ª×˜×ª×›××˜××›××›××˜××¡×›×›×Ÿ××××3×××’×˜×˜×˜××˜×˜×˜×›××¥×’×™×¢×¥××Ÿ×Ÿ×Ÿ×˜×’×¥×Ÿ×’×××™×Ÿ×××¥×Ÿ×¥××Ÿ×¥×˜×Ÿ×˜×›×Ÿ×¥×Ÿ9×˜×™×˜×—×˜×˜×˜×Ÿ×××™××™×’×’×™×™×’×’×’××˜×™4×¢××Ÿ×Ÿ×ª×¢×™×™××›×—×›×›×—×—×›×—×›×—×—×›×›×›×™×—×’×™×›×™×¥×™×¥×™×—×›9××›×—×¥×¥×¥×¥×—3×¤××™×˜×¥×™×™×¥×¥×˜30×33×˜×™3×¥×¥×™×˜×¥××˜×™×¥×˜×˜×—×˜×¥×™×˜9×Ÿ×™×›×¥×¥×›0×××¥×¥×¥×™×™33×™×¥×™×™×™×¥×’×Ÿ×Ÿ××Ÿ×¡×¥×™×™×¡××¡××¡×Ÿ×‘×××¡×Ÿ×Ÿ×™×™×¡×¡×—×›×¡×—×™×—×›×—×™×™×™×›××—×’×’×’×××××××™×™×™×›×¥×™×¥×¥×™× ×™×¡××¨×—×¥×›×¨×¨2××¢×¢×™×™×™××××ª×”×›3×™×¥3×™×¥×™3×›333×’3×’×¥×™×™333×’×™22×’×’×’×¥2×’×™×’×¥×™×–×™×™×™×™×™×™×™×™×™×™×›×™×¥×’×¥×¥×™×’×¥×›3×¥×™×›×ª×™×’×ª×™×”×™×’×¥2×™×”×ª×’×’×’3××™×¥× ×š×”×™×ª×”×¥×™×’×™×¡×ª×™×”×¥×’×”×™×’×”×”×ª×’×’×’×”×ª×¢×›×›×›×¥×”×’×ª×ª×ª×™×’×™×™×ª×’×”×ª×’3×¥×™×’×’3×™×”×›×ª×”×–×¥×”×’×’×™×”×ª×’×’×’×™×¨×¨×¨×¨×™33×›×¡×ª×™×™×™×›×›×›×™×™×™3×™×™×™×™×›×™×™×¡×¥×’×’×’×’×‘×”×¡×‘×‘×”×™× × ×œ×œ×’×¡×”×”× × ×ž×¦×ž×‘×¡× ×›×³×’×’×’×××•×˜Ö¼×˜×××ŸÖ¼×˜×Ÿ×ª×›×³×†×†×³×†×›×ª×³×ª×³×ª×ª×³×ª×³×ª×³×ª×›×’Ö¼×›×’×ª×›×ª×³×†×†×´×Ÿ×›9×³×ª×³×†×³×†×ª×™×†×™×†×ª×›×³7×“×ª×’8×“9×’9×ª×†×†×ª×’×†×’×’×ª×’Ö¼×›×³×ª×™×†×³8×’7×’9×›8×9×’89×˜9×˜×•Ö¼×›×ª×†×›×ª×³×›×ª×›×†×›×ª×’×†×³×ª×›×†×³×ª×ª×³×ª×›8×›×ª×›×³×ª×³8×³9×™97×˜9×³8×³8×“9×Ÿ7×›9×˜×’9×›9×˜×•8×’×›×ª×‘×œ×œ×‘×œ×‘×‘×œ×´×‘×´×‘×‘×´×–×™×–×´×¡×´×¡×´×‘×œ×‘×´×‘×œ×‘×´×‘×œ×¡×–×–×¡×œ×¡×´×¡×œ×›×œ×‘×´×‘×œ×œ×‘×œ×‘×´×¡×œ×¡×´×¡×™×¡×œ×‘×œ×‘×œ×‘×´×¡×‘×´×‘×´×¡×œ×‘×œ×œ×³×œ×›×œ×´×›×³×œ×³×œ×›×œ×³×’×¡×´×¡×´×‘×‘×´×¡×‘×´×‘×´×¡×œ×›×œ×’×™×–×“×™×’×´×œ×œ×‘×™×™×š×™×´,× ,×™×š×³×ª×›×ª×›×›×´×–×´×¡×œ×œ×´×›×³×š×³×œ×›×’×œ×’×•×›×›×‘× ×œ×´×‘×¡×¡×™×™×™×”×”×”×´×´×™×´×œ×´×œ×´×´×´×œ×™×™×¢×›×•×Ÿ×Ÿ×Ÿ×¢×™×¢×¢×¢×¢×¢×¢×¢×¢×™×™×™×™×™×™×•×›×™×—×¢×™××—×—×—×™×›××Ÿ×¢×¢×¢×™×™×™×—×œ×¢×™×—×—×—×ª×ª×¢×•×Ÿ×Ÿ×Ÿ×•×˜×××›×¢×¢×¢×’×Ÿ×•×•×˜×•××Ÿ×××ª×ª×ª×˜×˜×•×•××ª××Ÿ×•×ª×¨××’×’×’×›×›×›×“×“×“×“×©×©×©×©×§×§×§×§×§×¨×¨××’×›×›×›×›×¢×¢×¢×¢×¢×™×™×™×™×—×ª×ª×ª×¥×¥×¤×¤×¤×¤×¤×¥×¥××Ÿ×•×˜`Â°Â®Â®Â¥Â¥_`â€¢â€¢Â¥Â¥Â¥Â°Â¥â€¢â€¢Ã·Ã—Ã—Ã·^Â°`Â¥Â°_Â´_Â¥Â®Â®Â®Â®Â®Â®Â¥Â¥â€¢Â®Â®Â®Â®Â®Â°_+[[+[{[[[[]]][+[+Â´Â´Â´Â´Â´Â´__Â¥Â®Â®^Ã—-Ã—
Start a new codespace

Click the Code button on your repository's landing page.
Click the Codespaces tab.
Click Create codespaces on main to create the codespace.
After the codespace has initialized there will be a terminal present.
Verify the GitHub Actions Importer CLI is installed and working. More information on the GitHub Actions Importer extension for the official GitHub CLI can be found here.

Run the following command in the codespace terminal:

gh actions-importer version
Verify the output is similar to below.

$ gh actions-importer version
gh version 2.14.3 (2022-07-26)
gh actions-importer        github/gh-actions-importer v0.1.12
actions-importer/cli       unknown
If gh actions-importer version did not produce similar output, please refer to the troubleshooting section.

Bootstrap a GitLab server
Execute the GitLab setup script that will start a container with GitLab running inside of it. The script should be executed when starting a new codespace or restarting an existing one.

Run the following command from the codespace terminal:

./gitlab/bootstrap/setup.sh
After some time, a pop-up box should appear with a link to the URL for your GitLab server.

You can also access the URL by going to the Ports tab in your terminal. Right-click the URL listed under the Local Address and click the Open in Browser tab.

Open the GitLab server in your browser and use the following credentials to authenticate:

Username: root
Password: actions-importer-labs!
Once authenticated, you should see a GitLab server with a few predefined pipelines in the actions-importer group.

Labs for GitLab
Perform the following labs to learn more about Actions migrations with GitHub Actions Importer:

Configure credentials for GitHub Actions Importer
Perform an audit on GitLab pipelines
Forecast potential build runner usage
Perform a dry-run migration of a GitLab pipeline
Use custom transformers to customize GitHub Actions Importer's behavior
Perform a production migration of a GitLab pipeline
Troubleshoot the GitHub Actions Importer CLI
The CLI extension for GitHub Actions Importer can be manually installed by following these steps:

Verify you are in the codespace terminal

Run this command from within the codespace terminal:

gh extension install github/gh-actions-importer
Verify the result of the install contains:

$ gh extension install github/gh-actions-importer
âœ“ Installed extension github/gh-actions-importer
Verify GitHub Actions Importer CLI extension is installed and working by running the following command from the codespace terminal:

gh actions-importer version
**To Reproduce**
Steps to reproduce the behavior:
1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

**Expected behavior**
A clear and concise description of what you expected to happen.

**Screenshots**
If applicable, add screenshots to help explain your problem.

**Desktop (please complete the following information):**
 - OS: [e.g. iOS]
 - Browser [e.g. chrome, safari]
 - Version [e.g. 22]

**Smartphone (please complete the following information):**
 - Device: [e.g. iPhone6]
 - OS: [e.g. iOS8.1]
 - Browser [e.g. stock browser, safari]
 - Version [e.g. 22]

**Additional context**
Add any other context about the problem here.**Is your feature request related to a problem? Please describe.**
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...]

**Describe the solution you'd like**
A clear and concise description of what you want to happen.

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.https://github.com/bradford80USA/unamed/actions/workflows/azure-container-webapp.yml1998350Program Overview
Privileges of Membership
Over 1300 organizations and individuals parti-
cipate in the JCP program. While there are no
obligatory duties, members have the opportunity
to influence the evolution of Java technology
through the development of Java Specification
Requests (JSR).
Members can license their Java specifications
under a variety of licenses, including open source
options. Anyone must be able to create an indepen-
dent implementation as long as they license and pass the
TCK to ensure compatibility. Members must also
make the option available to license the TCK and
RI separately. In addition, individuals, educational
organizations, and qualified nonprofits must have
access to the TCKs free of charge.
Successful Members:
â€¢ Review proposed JSRs and drafts
â€¢ Submit JSRs
â€¢ Nominate themselves or others to serve
on Expert Groups, which create or revise
specifications
â€¢ Build independent implementations
â€¢ Vote on EC membership ballots
â€¢ Nominate themselves for an EC seat
Members of an Expert Group may also:
â€¢ Serve as the Specification Lead of an
Expert Group
â€¢ Select others to join their Expert Group
â€¢ Use feedback from members and the public
to improve the quality of a specification
â€¢ Complete a specification, its RI, and its
associated TCK
â€¢ Maintain a specification after it is written
How to Become a Member
A person or organization can become a member
by signing the Java Specification Participation
Agreement (JSPA). This agreement between an
organization or individual and Oracle establishes
each memberâ€™s rights and obligations when partici-
pating in the JCP program. To cover costs, the JSPA
charges a nominal fee for commercial entities, but it
is free for Java User Groups and individuals.
The Java Specification Review Process
Currently, over 350 JSRs are in development.
A specification follows four major steps as it
progresses through the process, as shown in
the timeline.
1. INITIATION: A specification is initiated by one or
more members and approved for development
by the Executive Committee.
2. EARLY DRAFT: A group of experts is formed to
draft the specification for the public, community
and the Executive Committee to review. The
Expert Group uses feedback from the review to
revise the specification.
3. PUBLIC DRAFT: The draft is posted on the Internet
for a second review by the public. The Expert
Group uses the feedback to refine the document.
The Executive Committee decides if the draft
should proceed to the next step. The Specification
Lead ensures that the RI and its associated TCK
are completed before sending the specification to
the Executive Committee for final approval.
Java Community Process Program Overview
The Java Community Process (JCP) program is the formalization of the open, inclusive
process that has been used since 1998 to develop and revise Java technology specifications,
reference implementations (RI), and technology compatibility kits (TCK). Jav1300
  Start a new codespace.

Click the Code button on your repository's landing page.
Click the Codespaces tab.
Click Create codespaces on main to create the codespace.
After the codespace has initialized there will be a terminal present.
Verify the GitHub Actions Importer CLI is installed and working. More information on the GitHub Actions Importer extension for the official GitHub CLI can be found here.

Run the following command in the codespace terminal:

gh actions-importer version
Verify the output is similar to below.

$ gh actions-importer version
gh version 2.14.3 (2022-07-26)
gh actions-importer        github/gh-actions-importer v0.1.12
actions-importer/cli       unknown
If gh actions-importer version did not produce similar output, please refer to the troubleshooting section.

Bootstrap your Azure DevOps organization
Create an Azure DevOps personal access token (PAT):

Navigate to your existing organization (https://dev.azure.com/:organization) in your browser.
In the top right corner of the screen, click User settings.
Click Personal access tokens.
Select + New Token
Name your token, select the organization where you want to use the token, and set your token to automatically expire after a set number of days.
Select the following scopes (you may need to select Show all scopes at the bottom of the page to reveal all scopes):
Agents Pool: Read
Build: Read & execute
Code: Read & write
Project and Team: Read, write, & manage
Release: Read
Service Connections: Read
Task Groups: Read
Variable Groups: Read
Click Create.
Copy the generated API token and save it in a safe location.
Execute the Azure DevOps setup script that will create a new Azure DevOps project in your organization to be used in the following labs. This script should only be run once.

Run the following command from the codespace terminal, replacing the values accordingly:

:organization: the name of your existing Azure DevOps organization
:project: the name of the project to be created in your Azure DevOps organization
:access_token: the PAT created in step 1 above
./azure_devops/bootstrap/setup --organization :organization --project :project --access-token :access-token
Open the newly created Azure DevOps project in your browser (https://dev.azure.com/:organization/:project)

Once authenticated, you will see an Azure DevOps project with a few predefined pipelines.
Labs for Azure DevOps
Perform the following labs to learn how to migrate Azure DevOps pipelines to GitHub Actions using GitHub Actions Importer:

Configure credentials for GitHub Actions Importer
Perform an audit of an Azure DevOps project
Forecast potential build runner usage
Perform a dry-run migration of an Azure DevOps pipeline
Use custom transformers to customize GitHub Actions Importer's behavior
Perform a production migration of a Azure DevOps pipeline
Troubleshoot the GitHub Actions Importer CLI
The CLI extension for GitHub Actions Importer can be manually installed by following these steps:

Verify you are in the codespace terminal

Run this command from within the codespace terminal:

gh extension install github/gh-actions-importer
Verify the result of the install contains:

$ gh extension install github/gh-actions-importer
âœ“ Installed extension github/gh-actions-importer
Verify GitHub Actions Importer CLI extension is installed and working by running the following command from the codespace terminal:

gh actions-importer versionhttps://dev.azure.com/:organizationhttps://dev.azure.com/:organization/:project1{"$id":"1","innerException":null,"message":"A potentially dangerous Request.Path value was detected from the client (:).","typeName":"System.Web.HttpException, System.Web","typeKey":"HttpException","errorCode":0,"eventId":0}
Integrations are tools that extend GitHub's functionality. Integrations can do things on GitHub like open issues, comment on pull requests, and manage projects. They can also do things outside of GitHub based on events that happen on GitHub. For example, an integration can post on Slack when an issue is opened on GitHub.

Many integrations are GitHub Apps, GitHub Actions workflows, or custom actions for GitHub Actions workflows.

    GitHub Apps are integrations that run on the app owner's server or on a user device. For more information, see "About creating GitHub Apps."
    GitHub Actions workflows are workflows that run when specific events occur on GitHub. For more information, see "Understanding GitHub Actions."
    Custom actions are code that can be executed by a GitHub Actions workflow. For more information, see "About custom actions."

Your integration can use GitHub's API to fetch data and make changes to data on GitHub. GitHub has a REST API and a GraphQL API. For more information, see:

    "Comparing GitHub's REST API and GraphQL API"
    "GitHub REST API documentation"
    "GitHub GraphQL API documentation"

Your integration can use webhooks to learn when specific events happen on GitHub. For more information, see "About webhooks."

If your integration is a GitHub App or custom action, you can publish your integration on GitHub Marketplace. For more information, see "About GitHub Marketplace for apps" and "Publishing actions in GitHub MarkeStart a new codespace.

Click the Code button on your repository's landing page.
Click the Codespaces tab.
Click Create codespaces on main to create the codespace.
After the codespace has initialized there will be a terminal present.
Verify the GitHub Actions Importer CLI is installed and working. More information on the GitHub Actions Importer extension for the official GitHub CLI can be found here.

Run the following command in the codespace terminal:

gh actions-importer version
Verify the output is similar to below.

$ gh actions-importer version
gh version 2.14.3 (2022-07-26)
gh actions-importer        github/gh-actions-importer v0.1.12
actions-importer/cli       unknown
If gh actions-importer version did not produce similar output, please refer to the troubleshooting section.

Bootstrap your Azure DevOps organization
Create an Azure DevOps personal access token (PAT):

Navigate to your existing organization (https://dev.azure.com/:organization) in your browser.
In the top right corner of the screen, click User settings.
Click Personal access tokens.
Select + New Token
Name your token, select the organization where you want to use the token, and set your token to automatically expire after a set number of days.
Select the following scopes (you may need to select Show all scopes at the bottom of the page to reveal all scopes):
Agents Pool: Read
Build: Read & execute
Code: Read & write
Project and Team: Read, write, & manage
Release: Read
Service Connections: Read
Task Groups: Read
Variable Groups: Read
Click Create.
Copy the generated API token and save it in a safe location.
Execute the Azure DevOps setup script that will create a new Azure DevOps project in your organization to be used in the following labs. This script should only be run once.

Run the following command from the codespace terminal, replacing the values accordingly:

:organization: the name of your existing Azure DevOps organization
:project: the name of the project to be created in your Azure DevOps organization
:access_token: the PAT created in step 1 above
./azure_devops/bootstrap/setup --organization :organization --project :project --access-token :access-token
Open the newly created Azure DevOps project in your browser (https://dev.azure.com/:organization/:project)

Once authenticated, you will see an Azure DevOps project with a few predefined pipelines.
Labs for Azure DevOps
Perform the following labs to learn how to migrate Azure DevOps pipelines to GitHub Actions using GitHub Actions Importer:

Configure credentials for GitHub Actions Importer
Perform an audit of an Azure DevOps project
Forecast potential build runner usage
Perform a dry-run migration of an Azure DevOps pipeline
Use custom transformers to customize GitHub Actions Importer's behavior
Perform a production migration of a Azure DevOps pipeline
Troubleshoot the GitHub Actions Importer CLI
The CLI extension for GitHub Actions Importer can be manually installed by following these steps:

Verify you are in the codespace terminal

Run this command from within the codespace terminal:

gh extension install github/gh-actions-importer
Verify the result of the install contains:

$ gh extension install github/gh-actions-importer
âœ“ Installed extension github/gh-actions-importer
Verify GitHub Actions Importer CLI extension is installed and working by running the following command from the codespace terminal:

gh actions-importer versionhttps://dev.azure.com/:organization1
In this lab, you will build upon the dry-run command to override GitHub Actions Importer's default behavior and customize the converted workflow using "custom transformers." Custom transformers can be used to:

Convert items that are not automatically converted.
Convert items that were automatically converted using different actions.
Convert environment variable values differently.
Convert references to runners to use a different runner name in GitHub Actions.
Prerequisites
Followed the steps here to set up your GitHub Codespaces environment.
Completed the configure lab.
Completed the dry-run lab.
Perform a dry run
You will be performing a dry-run command to inspect the workflow that is converted by default. Run the following command within the codespace terminal:

gh actions-importer dry-run bitbucket --output-dir tmp/dry-run --workspace actions-importer --repository node-deploy --source-file-path ./bitbucket/bootstrap/source_files/node_deploy.yml
The converted workflow that is generated by the above command can be seen below:

Converted workflow ðŸ‘‡
Note: You can refer to the previous lab to learn about the fundamentals of the dry-run command.

Custom transformers for an unknown step
The converted workflow above contains a atlassian/unknown-azure-deploy step that was not automatically converted. Let's write a custom transformer to handle this unknown pipe!

Let's answer the following questions before proceeding to write a custom transformer.

What is the "identifier" of the step to customize? This should be the identifier from the comment in the workflow without the version, or in other words the name of the pipe.
atlassian/unknown-azure-deploy
What is the desired Actions syntax to use instead?
Upon conducting some research, you've discovered that the Azure Web App and Azure Login actions available in the marketplace offer comparable functionality.
- uses: azure/login@v1.4.6
  with:
    creds: "${{ secrets.AZURE_CREDENTIALS }}"
- uses: azure/webapps-deploy@v2.2.5
  with:
    app-name: my-site
    package: my-package.zip
    resource-group-name: "$AZURE_RESOURCE_GROUP"
Now you can begin to write the custom transformer. Custom transformers use a DSL built on top of Ruby and should be defined in a file with the .rb file extension. You can create this file by running the following command in your codespace terminal:

touch transformers.rb && code transformersmy-package.ziptransformers.rbIntegrations are tools that extend GitHub's functionality. Integrations can do things on GitHub like open issues, comment on pull requests, and manage projects. They can also do things outside of GitHub based on events that happen on GitHub. For example, an integration can post on Slack when an issue is opened on GitHub.

Many integrations are GitHub Apps, GitHub Actions workflows, or custom actions for GitHub Actions workflows.

    GitHub Apps are integrations that run on the app owner's server or on a user device. For more information, see "About creating GitHub Apps."
    GitHub Actions workflows are workflows that run when specific events occur on GitHub. For more information, see "Understanding GitHub Actions."
    Custom actions are code that can be executed by a GitHub Actions workflow. For more information, see "About custom actions."

Your integration can use GitHub's API to fetch data and make changes to data on GitHub. GitHub has a REST API and a GraphQL API. For more information, see:

    "Comparing GitHub's REST API and GraphQL API"
    "GitHub REST API documentation"
    "GitHub GraphQL API documentation"

Your integration can use webhooks to learn when specific events happen on GitHub. For more information, see "About webhooks."

If your integration is a GitHub App or custom action, you can publish your integration on GitHub Marketplace. For more information, see "About GitHub Marketplace for apps" and "Publishing actions in GitHub MarkeStart a new codespace.

Click the Code button on your repository's landing page.
Click the Codespaces tab.
Click Create codespaces on main to create the codespace.
After the codespace has initialized there will be a terminal present.
Verify the GitHub Actions Importer CLI is installed and working. More information on the GitHub Actions Importer extension for the official GitHub CLI can be found here.

Run the following command in the codespace terminal:

gh actions-importer version
Verify the output is similar to below.

$ gh actions-importer version
gh version 2.14.3 (2022-07-26)
gh actions-importer        github/gh-actions-importer v0.1.12
actions-importer/cli       unknown
If gh actions-importer version did not produce similar output, please refer to the troubleshooting section.

Bootstrap your Azure DevOps organization
Create an Azure DevOps personal access token (PAT):

Navigate to your existing organization (https://dev.azure.com/:organization) in your browser.
In the top right corner of the screen, click User settings.
Click Personal access tokens.
Select + New Token
Name your token, select the organization where you want to use the token, and set your token to automatically expire after a set number of days.
Select the following scopes (you may need to select Show all scopes at the bottom of the page to reveal all scopes):
Agents Pool: Read
Build: Read & execute
Code: Read & write
Project and Team: Read, write, & manage
Release: Read
Service Connections: Read
Task Groups: Read
Variable Groups: Read
Click Create.
Copy the generated API token and save it in a safe location.
Execute the Azure DevOps setup script that will create a new Azure DevOps project in your organization to be used in the following labs. This script should only be run once.

Run the following command from the codespace terminal, replacing the values accordingly:

:organization: the name of your existing Azure DevOps organization
:project: the name of the project to be created in your Azure DevOps organization
:access_token: the PAT created in step 1 above
./azure_devops/bootstrap/setup --organization :organization --project :project --access-token :access-token
Open the newly created Azure DevOps project in your browser (https://dev.azure.com/:organization/:project)

Once authenticated, you will see an Azure DevOps project with a few predefined pipelines.
Labs for Azure DevOps
Perform the following labs to learn how to migrate Azure DevOps pipelines to GitHub Actions using GitHub Actions Importer:

Configure credentials for GitHub Actions Importer
Perform an audit of an Azure DevOps project
Forecast potential build runner usage
Perform a dry-run migration of an Azure DevOps pipeline
Use custom transformers to customize GitHub Actions Importer's behavior
Perform a production migration of a Azure DevOps pipeline
Troubleshoot the GitHub Actions Importer CLI
The CLI extension for GitHub Actions Importer can be manually installed by following these steps:

Verify you are in the codespace terminal

Run this command from within the codespace terminal:

gh extension install github/gh-actions-importer
Verify the result of the install contains:

$ gh extension install github/gh-actions-importer
âœ“ Installed extension github/gh-actions-importer
Verify GitHub Actions Importer CLI extension is installed and working by running the following command from the codespace terminal:

gh actions-importer versionhttps://dev.azure.com/:organization
Start a new codespace.

Click the Code button on your repository's landing page.
Click the Codespaces tab.
Click Create codespaces on main to create the codespace.
After the codespace has initialized there will be a terminal present.
Verify the GitHub Actions Importer CLI is installed and working. More information on the GitHub Actions Importer extension for the official GitHub CLI can be found here.

Run the following command in the codespace terminal:

gh actions-importer version
Verify the output is similar to below.

$ gh actions-importer version
gh version 2.14.3 (2022-07-26)
gh actions-importer        github/gh-actions-importer v0.1.12
actions-importer/cli       unknown
If gh actions-importer version did not produce similar output, please refer to the troubleshooting section.

Bootstrap your Azure DevOps organization
Create an Azure DevOps personal access token (PAT):

Navigate to your existing organization (https://dev.azure.com/:organization) in your browser.
In the top right corner of the screen, click User settings.
Click Personal access tokens.
Select + New Token
Name your token, select the organization where you want to use the token, and set your token to automatically expire after a set number of days.
Select the following scopes (you may need to select Show all scopes at the bottom of the page to reveal all scopes):
Agents Pool: Read
Build: Read & execute
Code: Read & write
Project and Team: Read, write, & manage
Release: Read
Service Connections: Read
Task Groups: Read
Variable Groups: Read
Click Create.
Copy the generated API token and save it in a safe location.
Execute the Azure DevOps setup script that will create a new Azure DevOps project in your organization to be used in the following labs. This script should only be run once.

Run the following command from the codespace terminal, replacing the values accordingly:

:organization: the name of your existing Azure DevOps organization
:project: the name of the project to be created in your Azure DevOps organization
:access_token: the PAT created in step 1 above
./azure_devops/bootstrap/setup --organization :organization --project :project --access-token :access-token
Open the newly created Azure DevOps project in your browser (https://dev.azure.com/:organization/:project)

Once authenticated, you will see an Azure DevOps project with a few predefined pipelines.
Labs for Azure DevOps
Perform the following labs to learn how to migrate Azure DevOps pipelines to GitHub Actions using GitHub Actions Importer:

Configure credentials for GitHub Actions Importer
Perform an audit of an Azure DevOps project
Forecast potential build runner usage
Perform a dry-run migration of an Azure DevOps pipeline
Use custom transformers to customize GitHub Actions Importer's behavior
Perform a production migration of a Azure DevOps pipeline
Troubleshoot the GitHub Actions Importer CLI
The CLI extension for GitHub Actions Importer can be manually installed by following these steps:

Verify you are in the codespace terminal

Run this command from within the codespace terminal:

gh extension install github/gh-actions-importer
Verify the result of the install contains:

$ gh extension install github/gh-actions-importer
âœ“ Installed extension github/gh-actions-importer
Verify GitHub Actions Importer CLI extension is installed and working by running the following command from the codespace terminal:

gh actions-importer versionStart a new codespace.

Click the Code button on your repository's landing page.
Click the Codespaces tab.
Click Create codespaces on main to create the codespace.
After the codespace has initialized there will be a terminal present.
Verify the GitHub Actions Importer CLI is installed and working. More information on the GitHub Actions Importer extension for the official GitHub CLI can be found here.

Run the following command in the codespace terminal:

gh actions-importer version
Verify the output is similar to below.

$ gh actions-importer version
gh version 2.14.3 (2022-07-26)
gh actions-importer        github/gh-actions-importer v0.1.12
actions-importer/cli       unknown
If gh actions-importer version did not produce similar output, please refer to the troubleshooting section.

Bootstrap your Azure DevOps organization
Create an Azure DevOps personal access token (PAT):

Navigate to your existing organization (https://dev.azure.com/:organization) in your browser.
In the top right corner of the screen, click User settings.
Click Personal access tokens.
Select + New Token
Name your token, select the organization where you want to use the token, and set your token to automatically expire after a set number of days.
Select the following scopes (you may need to select Show all scopes at the bottom of the page to reveal all scopes):
Agents Pool: Read
Build: Read & execute
Code: Read & write
Project and Team: Read, write, & manage
Release: Read
Service Connections: Read
Task Groups: Read
Variable Groups: Read
Click Create.
Copy the generated API token and save it in a safe location.
Execute the Azure DevOps setup script that will create a new Azure DevOps project in your organization to be used in the following labs. This script should only be run once.

Run the following command from the codespace terminal, replacing the values accordingly:

:organization: the name of your existing Azure DevOps organization
:project: the name of the project to be created in your Azure DevOps organization
:access_token: the PAT created in step 1 above
./azure_devops/bootstrap/setup --organization :organization --project :project --access-token :access-token
Open the newly created Azure DevOps project in your browser (https://dev.azure.com/:organization/:project)

Once authenticated, you will see an Azure DevOps project with a few predefined pipelines.
Labs for Azure DevOps
Perform the following labs to learn how to migrate Azure DevOps pipelines to GitHub Actions using GitHub Actions Importer:

Configure credentials for GitHub Actions Importer
Perform an audit of an Azure DevOps project
Forecast potential build runner usage
Perform a dry-run migration of an Azure DevOps pipeline
Use custom transformers to customize GitHub Actions Importer's behavior
Perform a production migration of a Azure DevOps pipeline
Troubleshoot the GitHub Actions Importer CLI
The CLI extension for GitHub Actions Importer can be manually installed by following these steps:

Verify you are in the codespace terminal

Run this command from within the codespace terminal:

gh extension install github/gh-actions-importer
Verify the result of the install contains:

$ gh extension install github/gh-actions-importer
âœ“ Installed extension github/gh-actions-importer
Verify GitHub Actions Importer CLI extension is installed and working by running the following command from the codespace terminal:

gh actions-importer versionhttps://dev.azure.com/:organization1https://dev.azure.com/:organization/:projectIn this lab, you will build upon the dry-run command to override GitHub Actions Importer's default behavior and customize the converted workflow using "custom transformers." Custom transformers can be used to:

Convert items that are not automatically converted.
Convert items that were automatically converted using different actions.
Convert environment variable values differently.
Convert references to runners to use a different runner name in GitHub Actions.
Prerequisites
Followed the steps here to set up your GitHub Codespaces environment.
Completed the configure lab.
Completed the dry-run lab.
Perform a dry run
You will be performing a dry-run command to inspect the workflow that is converted by default. Run the following command within the codespace terminal:

gh actions-importer dry-run bitbucket --output-dir tmp/dry-run --workspace actions-importer --repository node-deploy --source-file-path ./bitbucket/bootstrap/source_files/node_deploy.yml
The converted workflow that is generated by the above command can be seen below:

Converted workflow ðŸ‘‡
Note: You can refer to the previous lab to learn about the fundamentals of the dry-run command.

Custom transformers for an unknown step
The converted workflow above contains a atlassian/unknown-azure-deploy step that was not automatically converted. Let's write a custom transformer to handle this unknown pipe!

Let's answer the following questions before proceeding to write a custom transformer.

What is the "identifier" of the step to customize? This should be the identifier from the comment in the workflow without the version, or in other words the name of the pipe.
atlassian/unknown-azure-deploy
What is the desired Actions syntax to use instead?
Upon conducting some research, you've discovered that the Azure Web App and Azure Login actions available in the marketplace offer comparable functionality.
- uses: azure/login@v1.4.6
  with:
    creds: "${{ secrets.AZURE_CREDENTIALS }}"
- uses: azure/webapps-deploy@v2.2.5
  with:
    app-name: my-site
    package: my-package.zip
    resource-group-name: "$AZURE_RESOURCE_GROUP"
Now you can begin to write the custom transformer. Custom transformers use a DSL built on top of Ruby and should be defined in a file with the .rb file extension. You can create this file by running the following command in your codespace terminal:

touch transformers.rb && code transformersmy-package.ziptransformers.rbIntegrations are tools that extend GitHub's functionality. Integrations can do things on GitHub like open issues, comment on pull requests, and manage projects. They can also do things outside of GitHub based on events that happen on GitHub. For example, an integration can post on Slack when an issue is opened on GitHub.

Many integrations are GitHub Apps, GitHub Actions workflows, or custom actions for GitHub Actions workflows.

    GitHub Apps are integrations that run on the app owner's server or on a user device. For more information, see "About creating GitHub Apps."
    GitHub Actions workflows are workflows that run when specific events occur on GitHub. For more information, see "Understanding GitHub Actions."
    Custom actions are code that can be executed by a GitHub Actions workflow. For more information, see "About custom actions."

Your integration can use GitHub's API to fetch data and make changes to data on GitHub. GitHub has a REST API and a GraphQL API. For more information, see:

    "Comparing GitHub's REST API and GraphQL API"
    "GitHub REST API documentation"
    "GitHub GraphQL API documentation"

Your integration can use webhooks to learn when specific events happen on GitHub. For more information, see "About webhooks."

If your integration is a GitHub App or custom action, you can publish your integration on GitHub Marketplace. For more information, see "About GitHub Marketplace for apps" and "Publishing actions in GitHub Marke{"$id":"1","innerException":null,"message":"A potentially dangerous Request.Path value was detected from the client (:).","typeName":"System.Web.HttpException, System.Web","typeKey":"HttpException","errorCode":0,"eventId":0}
        "3ds",
        "3g2",
        "3gp",
        "7z",
        "a",
        "aac",
        "adp",
        "ai",
        "aif",
        "aiff",
        "alz",
        "ape",
        "apk",
        "appimage",
        "ar",
        "arj",
        "asf",
        "au",
        "avi",
        "bak",
        "baml",
        "bh",
        "bin",
        "bk",
        "bmp",
        "btif",
        "bz2",
        "bzip2",
        "cab",
        "caf",
        "cgm",
        "class",
        "cmx",
        "cpio",
        "cr2",
        "cur",
        "dat",
        "dcm",
        "deb",
        "dex",
        "djvu",
        "dll",
        "dmg",
        "dng",
        "doc",
        "docm",
        "docx",
        "dot",
        "dotm",
        "dra",
        "DS_Store",
        "dsk",
        "dts",
        "dtshd",
        "dvb",
        "dwg",
        "dxf",
        "ecelp4800",
        "ecelp7470",
        "ecelp9600",
        "egg",
        "eol",
        "eot",
        "epub",
        "exe",
        "f4v",
        "fbs",
        "fh",
        "fla",
        "flac",
        "flatpak",
        "fli",
        "flv",
        "fpx",
        "fst",
        "fvt",
        "g3",
        "gh",
        "gif",
        "graffle",
        "gz",
        "gzip",
        "h261",
        "h263",
        "h264",
        "icns",
        "ico",
        "ief",
        "img",
        "ipa",
        "iso",
        "jar",
        "jpeg",
        "jpg",
        "jpgv",
        "jpm",
        "jxr",
        "key",
        "ktx",
        "lha",
        "lib",
        "lvp",
        "lz",
        "lzh",
        "lzma",
        "lzo",
        "m3u",
        "m4a",
        "m4v",
        "mar",
        "mdi",
        "mht",
        "mid",
        "midi",
        "mj2",
        "mka",
        "mkv",
        "mmr",
        "mng",
        "mobi",
        "mov",
        "movie",
        "mp3",
        "mp4",
        "mp4a",
        "mpeg",
        "mpg",
        "mpga",
        "mxu",
        "nef",
        "npx",
        "numbers",
        "nupkg",
        "o",
        "odp",
        "ods",
        "odt",
        "oga",
        "ogg",
        "ogv",
        "otf",
        "ott",
        "pages",
        "pbm",
        "pcx",
        "pdb",
        "pdf",
        "pea",
        "pgm",
        "pic",
        "png",
        "pnm",
        "pot",
        "potm",
        "potx",
        "ppa",
        "ppam",
        "ppm",
        "pps",
        "ppsm",
        "ppsx",
        "ppt",
        "pptm",
        "pptx",
        "psd",
        "pya",
        "pyc",
        "pyo",
        "pyv",
        "qt",
        "rar",
        "ras",
        "raw",
        "resources",
        "rgb",
        "rip",
        "rlc",
        "rmf",
        "rmvb",
        "rpm",
        "rtf",
        "rz",
        "s3m",
        "s7z",
        "scpt",
        "sgi",
        "shar",
        "snap",
        "sil",
        "sketch",
        "slk",
        "smv",
        "snk",
        "so",
        "stl",
        "suo",
        "sub",
        "swf",
        "tar",
        "tbz",
        "tbz2",
        "tga",
        "tgz",
        "thmx",
        "tif",
        "tiff",
        "tlz",
        "ttc",
        "ttf",
        "txz",
        "udf",
        "uvh",
        "uvi",
        "uvm",
        "uvp",
        "uvs",
        "uvu",
        "viv",
        "vob",
        "war",
        "wav",
        "wax",
        "wbmp",
        "wdp",
        "weba",
        "webm",
        "webp",
        "whl",
        "wim",
        "wm",
        "wma",
        "wmv",
        "wmx",
        "woff",
        "woff2",
        "wrm",
        "wvx",
        "xbm",
        "xif",
        "xla",
        "xlam",
        "xls",
        "xlsb",
        "xlsm",
        "xlsx",
        "xlt",
        "xltm",
        "xltx",
        "xm",
        "xmind",
        "xpi",
        "xpm",
        "xwd",
        "xz",
        "z",
        "zip",
        "zipx"        "3ds",        "3dm",$ npm install binary-extensionsUskingdom hearts 
systems
zkSync currently can be launched on any *nix operating system (e.g. any linux distribution or MacOS).

If you're using Windows, then make sure to use WSL 2, since WSL 1 is known to cause troubles.

Additionally, if you are going to use WSL 2, make sure that your project is located in the linux filesystem, since accessing NTFS partitions from inside of WSL is very slow.

If you're using MacOS with an ARM processor (e.g. M1/M2), make sure that you are working in the native environment (e.g. your terminal and IDE don't run in Rosetta, and your toolchain is native). Trying to work with zkSync code via Rosetta may cause problems that are hard to spot and debug, so make sure to check everything before you start.

If you are a NixOS user or would like to have a reproducible environment, skip to the section about nix.

Docker
Install docker. It is recommended to follow the instructions from the official site.

Note: currently official site proposes using Docker Desktop for linux, which is a GUI tool with plenty of quirks. If you want to only have CLI tool, you need the docker-ce package and you can follow this guide for Ubuntu.

Installing docker via snap or from the default repository can cause troubles.

You need to install both docker and docker-compose.

Note: docker-compose is installed automatically with Docker Desktop.

Note: On linux you may encounter the following error when youâ€™ll try to work with zksync:

ERROR: Couldn't connect to Docker daemon - you might need to run `docker-machine start default`.
If so, you do not need to install docker-machine. Most probably, it means that your user is not added to thedocker group. You can check it as follows:

docker-compose up # Should raise the same error.
sudo docker-compose up # Should start doing things.
If the first command fails, but the second succeeds, then you need to add your user to the docker group:

sudo usermod -a -G docker your_user_name
After that, you should logout and login again (user groups are refreshed after the login). The problem should be solved at this step.

If logging out does not help, restarting the computer should.

Node & Yarn
Install Node (requires version v18.18.0). Since our team attempts to always use the latest LTS version of Node.js, we suggest you to install nvm. It will allow you to update Node.js version easily in the future (by running nvm use in the root of the repository)
Install yarn (make sure to get version 1.22.19 - you can change the version by running yarn set version 1.22.19). Instructions can be found on the official site.
Check if yarn is installed by running yarn -v. If you face any problems when installing yarn, it might be the case that your package manager installed the wrong package.Make sure to thoroughly follow the instructions above on the official website. It contains a lot of troubleshooting guides in it.
Axel
Install axel for downloading keys:

On mac:

brew install axel
On debian-based linux:

sudo apt-get install axel
Check the version of axel with the following command:

axel --version
Make sure the version is higher than 2.17.10.

clang
In order to compile RocksDB, you must have LLVM available. On debian-based linux it can be installed as follows:

On linux:

sudo apt-get install build-essential pkg-config cmake clang lldb lld
On mac:

You need to have an up-to-date Xcode. You can install it directly from App Store. With Xcode command line tools, you get the Clang compiler installed by default. Thus, having XCode you don't need to install clang.

OpenSSL
Install OpenSSL:

On mac:

brew install openssl
On linux:

sudo apt-get install libssl-dev
Rust
Install the latest rust version.

Instructions can be found on the official site.

Verify the rust installation:

rustc --version
rustc 1.xx.y (xxxxxx 20xx-yy-zz) # Output may vary depending on actual version of rust
If you are using MacOS with ARM processor (e.g. M1/M2), make sure that you use an aarch64 toolchain. For example, when you run rustup show, you should see a similar input:

rustup show
Default host: aarch64-apple-darwin
rustup home:  /Users/user/.rustup

installed toolchains
--------------------

...

active toolchain
----------------

1.67.1-aarch64-apple-darwin (overridden by '/Users/user/workspace/zksync-era/rust-toolchain')
If you see x86_64 mentioned in the output, probably you're running (or used to run) your IDE/terminal in Rosetta. If that's the case, you should probably change the way you run terminal, and/or reinstall your IDE, and then reinstall the Rust toolchain as well.

Postgres
Install the latest postgres:

On mac:

brew install postgresql@14
On linux:

sudo apt-get install postgresql
Cargo nextest
cargo-nextest is the next-generation test runner for Rust projects. zk test rust uses cargo nextest by default.

cargo install cargo-nextest
SQLx CLI
SQLx is a Rust library we use to interact with Postgres, and its CLI is used to manage DB migrations and support several features of the library.

cargo install sqlx-cli --version 0.5.13
Solidity compiler solc
Install the latest solidity compiler.

brew install solidity
Alternatively, download a precompiled version and add it to your PATH.

Python
Most environments will have this preinstalled but if not, install Python.

Easier method using nix
Nix is a tool that can fetch exactly the right dependencies specified via hashes. The current config is Linux-only but it is likely that it can be adapted to Mac.

Install nix. Enable the nix command and flakes.

Install docker, rustup and use rust to install SQLx CLI like described above. If you are on NixOS, you also need to enable nix-ld.

Go to the zksync folder and run nix develop --impure. After it finishes, you are in a shell that has all the dependencies.

Environment
Edit the lines below and add them to your shell profile file (e.g. ~/.bash_profile, ~/.zshrc):

# Add path here:
export ZKSYNC_HOME=/path/to/zksync

export PATH=$ZKSYNC_HOME/bin:$PATH

# If you're like me, uncomment:
# cd $ZKSYNC_HOME
Tip: mold
Optionally, you may want to optimize the build time with the modern linker, mold.

This linker will speed up the build times, which can be pretty big for Rust binaries.

Follow the instructions in the repo in order to install it and enable for Rust.

Tip: Speeding up building RocksDB
By default, each time you compile rocksdb crate, it will compile required C++ sources from scratch. It can be avoided by using precompiled versions of library, and it will significantly improve your build times.

In order to do so, you can put compiled libraries to some persistent location, and add the following to your shell configuration file (e.g. .zshrc or .bashrc):

export ROCKSDB_LIB_DIR=<library location>
export SNAPPY_LIB_DIR=<library location>
Make sure that compiled libraries match the current version of RocksDB. One way to obtain them, is to compile the project in the usual way once, and then take built libraries from target/{debug,release}/build/librocksdb-sys-{some random1Node.jspackage.Makenpm update @coinbase/wallet-sdknpm outdated @coinbase/wallet-sdk{
  "dependencies": {
    "@coinbase/wallet-sdk": "^3.0.0"
  }
}yarn list @coinbase/wallet-sdkkingdom hearts 

 dependencies
Run npm install and verify if everything still works correctly
Merge your feature branch into V3
Create a new branch from V3 and name it with the version tag
Run npm run pre-publish and set new custom version
Update version in ConstantsUtil in @web3modal/scaffold-utils to the correct version
Create a new PR with Release Notes and merge into V3
Checkout V3 and run npm run publish:latest
Draft a new release in GitHub and create new tag
Click on Generate Change and only leave the link with difference. Paste in your changelog from PR.
Check Set as the last release and publish
npx create-wc-dapp@latest -y w3msystems
zkSync currently can be launched on any *nix operating system (e.g. any linux distribution or MacOS).

If you're using Windows, then make sure to use WSL 2, since WSL 1 is known to cause troubles.

Additionally, if you are going to use WSL 2, make sure that your project is located in the linux filesystem, since accessing NTFS partitions from inside of WSL is very slow.

If you're using MacOS with an ARM processor (e.g. M1/M2), make sure that you are working in the native environment (e.g. your terminal and IDE don't run in Rosetta, and your toolchain is native). Trying to work with zkSync code via Rosetta may cause problems that are hard to spot and debug, so make sure to check everything before you start.

If you are a NixOS user or would like to have a reproducible environment, skip to the section about nix.

Docker
Install docker. It is recommended to follow the instructions from the official site.

Note: currently official site proposes using Docker Desktop for linux, which is a GUI tool with plenty of quirks. If you want to only have CLI tool, you need the docker-ce package and you can follow this guide for Ubuntu.

Installing docker via snap or from the default repository can cause troubles.

You need to install both docker and docker-compose.

Note: docker-compose is installed automatically with Docker Desktop.

Note: On linux you may encounter the following error when youâ€™ll try to work with zksync:

ERROR: Couldn't connect to Docker daemon - you might need to run `docker-machine start default`.
If so, you do not need to install docker-machine. Most probably, it means that your user is not added to thedocker group. You can check it as follows:

docker-compose up # Should raise the same error.
sudo docker-compose up # Should start doing things.
If the first command fails, but the second succeeds, then you need to add your user to the docker group:

sudo usermod -a -G docker your_user_name
After that, you should logout and login again (user groups are refreshed after the login). The problem should be solved at this step.

If logging out does not help, restarting the computer should.

Node & Yarn
Install Node (requires version v18.18.0). Since our team attempts to always use the latest LTS version of Node.js, we suggest you to install nvm. It will allow you to update Node.js version easily in the future (by running nvm use in the root of the repository)
Install yarn (make sure to get version 1.22.19 - you can change the version by running yarn set version 1.22.19). Instructions can be found on the official site.
Check if yarn is installed by running yarn -v. If you face any problems when installing yarn, it might be the case that your package manager installed the wrong package.Make sure to thoroughly follow the instructions above on the official website. It contains a lot of troubleshooting guides in it.
Axel
Install axel for downloading keys:

On mac:

brew install axel
On debian-based linux:

sudo apt-get install axel
Check the version of axel with the following command:

axel --version
Make sure the version is higher than 2.17.10.

clang
In order to compile RocksDB, you must have LLVM available. On debian-based linux it can be installed as follows:

On linux:

sudo apt-get install build-essential pkg-config cmake clang lldb lld
On mac:

You need to have an up-to-date Xcode. You can install it directly from App Store. With Xcode command line tools, you get the Clang compiler installed by default. Thus, having XCode you don't need to install clang.

OpenSSL
Install OpenSSL:

On mac:

brew install openssl
On linux:

sudo apt-get install libssl-dev
Rust
Install the latest rust version.

Instructions can be found on the official site.

Verify the rust installation:

rustc --version
rustc 1.xx.y (xxxxxx 20xx-yy-zz) # Output may vary depending on actual version of rust
If you are using MacOS with ARM processor (e.g. M1/M2), make sure that you use an aarch64 toolchain. For example, when you run rustup show, you should see a similar input:

rustup show
Default host: aarch64-apple-darwin
rustup home:  /Users/user/.rustup

installed toolchains
--------------------

...

active toolchain
----------------

1.67.1-aarch64-apple-darwin (overridden by '/Users/user/workspace/zksync-era/rust-toolchain')
If you see x86_64 mentioned in the output, probably you're running (or used to run) your IDE/terminal in Rosetta. If that's the case, you should probably change the way you run terminal, and/or reinstall your IDE, and then reinstall the Rust toolchain as well.

Postgres
Install the latest postgres:

On mac:

brew install postgresql@14
On linux:

sudo apt-get install postgresql
Cargo nextest
cargo-nextest is the next-generation test runner for Rust projects. zk test rust uses cargo nextest by default.

cargo install cargo-nextest
SQLx CLI
SQLx is a Rust library we use to interact with Postgres, and its CLI is used to manage DB migrations and support several features of the library.

cargo install sqlx-cli --version 0.5.13
Solidity compiler solc
Install the latest solidity compiler.

brew install solidity
Alternatively, download a precompiled version and add it to your PATH.

Python
Most environments will have this preinstalled but if not, install Python.

Easier method using nix
Nix is a tool that can fetch exactly the right dependencies specified via hashes. The current config is Linux-only but it is likely that it can be adapted to Mac.

Install nix. Enable the nix command and flakes.

Install docker, rustup and use rust to install SQLx CLI like described above. If you are on NixOS, you also need to enable nix-ld.

Go to the zksync folder and run nix develop --impure. After it finishes, you are in a shell that has all the dependencies.

Environment
Edit the lines below and add them to your shell profile file (e.g. ~/.bash_profile, ~/.zshrc):

# Add path here:
export ZKSYNC_HOME=/path/to/zksync

export PATH=$ZKSYNC_HOME/bin:$PATH

# If you're like me, uncomment:
# cd $ZKSYNC_HOME
Tip: mold
Optionally, you may want to optimize the build time with the modern linker, mold.

This linker will speed up the build times, which can be pretty big for Rust binaries.

Follow the instructions in the repo in order to install it and enable for Rust.

Tip: Speeding up building RocksDB
By default, each time you compile rocksdb crate, it will compile required C++ sources from scratch. It can be avoided by using precompiled versions of library, and it will significantly improve your build times.

In order to do so, you can put compiled libraries to some persistent location, and add the following to your shell configuration file (e.g. .zshrc or .bashrc):

export ROCKSDB_LIB_DIR=<library location>
export SNAPPY_LIB_DIR=<library location>
Make sure that compiled libraries match the current version of RocksDB. One way to obtain them, is to compile the project in the usual way once, and then take built libraries from target/{debug,release}/build/librocksdb-sys-{some randompackage.MakeNode.jsnpm update @coinbase/wallet-sdk{
  "dependencies": {
    "@coinbase/wallet-sdk": "^3.0.0"
  }
}
, e.g., cat-2.png
### Image 3: <image>       % some image, e.g., cat-3.png
### Question: <question>   % What's the difference between the three cats?
### Answer: <answer>       % The colors of the three cats are different.
...
The training experience of DeepSpeed-VisualChat is straightforward and convenient. Here we give an example based on the CLIP visual encoder and the LLaMa-7B model:

git clone https://github.com/microsoft/DeepSpeedExamples.git
cd DeepSpeedExamples/applications/DeepSpeed-VisualChat/
pip install -r requirements.txt
cd training
bash training_scripts/run_7b.sh
The trained checkpoint will be automatically saved in a Hugging Face-compatible version and can be used to launch your own visual chat API:

cd ../chat
bash chat_scripts/run.sh # You need to change necessary variables, e.g, ckpt path
To support larger model inference, we have incorporated Hugging Face large model inference into our DeepSpeed-VisualChat API. Therefore, users can choose a different number of GPUs based on the GPU memory capacity and the model size.

Please refer to our GitHub Landing Page for more details.

7. Release: Try DeepSpeed-VisualChat today!
We are very excited to share that DeepSpeed-VisualChat is now open-sourced and available to the AI community.

To get started, please visit our GitHub page for DeepSpeed-VisualChat: GitHub Landing Page

We will continue to improve DeepSpeed-VisualChat with your feedback and support. Our roadmap shows currently supported features as well as ones that are planned for the future.

DeepSpeed-VisualChat is a component of the larger DeepSpeed ecosystem, which includes a range of Deep Learning systems and modeling technologies. To learn more,

Please visit our website for detailed blog posts, tutorials, and helpful documentation.
Follow us on our English X(Twitter), Japanese X(Twitter), and Chinese Zhihu for latest news on DeepSpeed.
We welcome your contributions to DeepSpeed! We encourage you to report issues, contribute PRs, and join discussions on the DeepSpeed GitHub page. Please see our contributing guide for more details. We are open to collaborations with universities, research labs, companies, such as those working together on deep learning research, applying DeepSpeed to empower real-world AI models and applications, and so on. For such requests (and other requests unsuitable for GitHub), please directly email to deepspeed-info@microsoft.com.

"Star" our DeepSpeed GitHub and DeepSpeedExamples GitHub cat-2.pngcat-3.pnghttps://github.com/microsoft/DeepSpeedExamples.gitrequirements.txtdeepspeed-info@microsoft.com  title={{DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via Multi-Modal Causal Attention}},
  author={Zhewei Yao and Xiaoxia Wu and Conglong Li and Minjia Zhang and Heyang Qin and Olatunji Ruwase and Ammar Ahmad Awan and Samyam Rajbhandari and Yuxiong He},
  journal={arXiv preprint arXiv:2309.14327},
  year={2023}
}
1. Overview
Large Language models (LLMs), such as GPT and LLaMa, have showcased exceptional prowess in a myriad of text generation and comprehension tasks, especially when subjected to zero-/few-shot learning, particularly after instructed fine-tuning. However, to equip AI agents for diverse tasks, one critical feature that needs to be incorporated is multi-modal capability; for instance, the AI agent should be able to read images, hear voices, watch videos, etc. This capability is largely absent in solely text-based LLMs.

Recently, one of the research/practice mainstreams has begun exploring the incorporation of visual capability into LLMs, especially enabling LLMs to understand images by inserting raw pictures (referred to as large visual language models, or LVLMs in short).

The main caveats of the majority of existing works are:

The focus is predominantly on tasks related to a single image, such as visual question answering and captioning, or on handling multiple images that require concurrent input. Neither approach adeptly manages interleaved image-and-text input.
The scalability of the system is limited to models with ~10B parameters, which is about an order of magnitude smaller than largest open-sourced models.
However, for a genuine AI chat agent, the content of inputs could be multiple images interleaved with text, a situation rarely addressed by current works. Also, the generation capability of LLMs grows quickly as the model size increases. Therefore, focusing system capability on ~10B models limits further exploration of the potential of LVLMs.

To resolve these issues, we are introducing DeepSpeed-VisualChat (see arxiv report for more details) with the following new features:

Fully Open-Sourced Multi-round Multi-image Framework with Unprecedented Scalability: DeepSpeed-VisualChat, one of the pioneering fully open-sourced frameworks, enables multi-round and multi-image dialogues, accommodating interleaved text-and-image inputs. We leverage DeepSpeed to enhance our training with a 2B visual encoder and a 70B LLaMA-2 decoder model, illustrating the remarkable scalability of our framework.
Multi-Modal Causal Attention (MMCA) We devise a novel MMCA for multi-modal models that computes attention weights independently across various modalities. MMCA achieves objectives analogous to conventional cross-attention mechanisms but offers enhanced causal attention interpretations for generative tasks, eliminating the need for additional modules or parameters. It also presents superior training data efficiency compared to standard causal attention.
Data Blending for Interleaved Inputs To facilitate conversations with interleaved modalities, DeepSpeed-VisualChat employs assorted data blending techniques on existing datasets, overcoming the shortage of interleaved text-and-image inputs in most available open-source datasets.
2 Model architecture overview
2 dependencies
Run npm install and verify if everything still works correctly
Merge your feature branch into V3
Create a new branch from V3 and name it with the version tag
Run npm run pre-publish and set new custom version
Update version in ConstantsUtil in @web3modal/scaffold-utils to the correct version
Create a new PR with Release Notes and merge into V3
Checkout V3 and run npm run publish:latest
Draft a new release in GitHub and create new tag
Click on Generate Change and only leave the link with difference. Paste in your changelog from PR.
Check Set as the last release and publishnpx create-wc-dapp@latest -y w3msystems
zkSync currently can be launched on any *nix operating system (e.g. any linux distribution or MacOS).

If you're using Windows, then make sure to use WSL 2, since WSL 1 is known to cause troubles.

Additionally, if you are going to use WSL 2, make sure that your project is located in the linux filesystem, since accessing NTFS partitions from inside of WSL is very slow.

If you're using MacOS with an ARM processor (e.g. M1/M2), make sure that you are working in the native environment (e.g. your terminal and IDE don't run in Rosetta, and your toolchain is native). Trying to work with zkSync code via Rosetta may cause problems that are hard to spot and debug, so make sure to check everything before you start.

If you are a NixOS user or would like to have a reproducible environment, skip to the section about nix.

Docker
Install docker. It is recommended to follow the instructions from the official site.

Note: currently official site proposes using Docker Desktop for linux, which is a GUI tool with plenty of quirks. If you want to only have CLI tool, you need the docker-ce package and you can follow this guide for Ubuntu.

Installing docker via snap or from the default repository can cause troubles.

You need to install both docker and docker-compose.

Note: docker-compose is installed automatically with Docker Desktop.

Note: On linux you may encounter the following error when youâ€™ll try to work with zksync:

ERROR: Couldn't connect to Docker daemon - you might need to run `docker-machine start default`.
If so, you do not need to install docker-machine. Most probably, it means that your user is not added to thedocker group. You can check it as follows:

docker-compose up # Should raise the same error.
sudo docker-compose up # Should start doing things.
If the first command fails, but the second succeeds, then you need to add your user to the docker group:

sudo usermod -a -G docker your_user_name
After that, you should logout and login again (user groups are refreshed after the login). The problem should be solved at this step.

If logging out does not help, restarting the computer should.

Node & Yarn
Install Node (requires version v18.18.0). Since our team attempts to always use the latest LTS version of Node.js, we suggest you to install nvm. It will allow you to update Node.js version easily in the future (by running nvm use in the root of the repository)
Install yarn (make sure to get version 1.22.19 - you can change the version by running yarn set version 1.22.19). Instructions can be found on the official site.
Check if yarn is installed by running yarn -v. If you face any problems when installing yarn, it might be the case that your package manager installed the wrong package.Make sure to thoroughly follow the instructions above on the official website. It contains a lot of troubleshooting guides in it.
Axel
Install axel for downloading keys:

On mac:

brew install axel
On debian-based linux:

sudo apt-get install axel
Check the version of axel with the following command:

axel --version
Make sure the version is higher than 2.17.10.

clang
In order to compile RocksDB, you must have LLVM available. On debian-based linux it can be installed as follows:

On linux:

sudo apt-get install build-essential pkg-config cmake clang lldb lld
On mac:

You need to have an up-to-date Xcode. You can install it directly from App Store. With Xcode command line tools, you get the Clang compiler installed by default. Thus, having XCode you don't need to install clang.

OpenSSL
Install OpenSSL:

On mac:

brew install openssl
On linux:

sudo apt-get install libssl-dev
Rust
Install the latest rust version.

Instructions can be found on the official site.

Verify the rust installation:

rustc --version
rustc 1.xx.y (xxxxxx 20xx-yy-zz) # Output may vary depending on actual version of rust
If you are using MacOS with ARM processor (e.g. M1/M2), make sure that you use an aarch64 toolchain. For example, when you run rustup show, you should see a similar input:

rustup show
Default host: aarch64-apple-darwin
rustup home:  /Users/user/.rustup

installed toolchains
--------------------

...

active toolchain
----------------

1.67.1-aarch64-apple-darwin (overridden by '/Users/user/workspace/zksync-era/rust-toolchain')
If you see x86_64 mentioned in the output, probably you're running (or used to run) your IDE/terminal in Rosetta. If that's the case, you should probably change the way you run terminal, and/or reinstall your IDE, and then reinstall the Rust toolchain as well.

Postgres
Install the latest postgres:

On mac:

brew install postgresql@14
On linux:

sudo apt-get install postgresql
Cargo nextest
cargo-nextest is the next-generation test runner for Rust projects. zk test rust uses cargo nextest by default.

cargo install cargo-nextest
SQLx CLI
SQLx is a Rust library we use to interact with Postgres, and its CLI is used to manage DB migrations and support several features of the library.

cargo install sqlx-cli --version 0.5.13
Solidity compiler solc
Install the latest solidity compiler.

brew install solidity
Alternatively, download a precompiled version and add it to your PATH.

Python
Most environments will have this preinstalled but if not, install Python.

Easier method using nix
Nix is a tool that can fetch exactly the right dependencies specified via hashes. The current config is Linux-only but it is likely that it can be adapted to Mac.

Install nix. Enable the nix command and flakes.

Install docker, rustup and use rust to install SQLx CLI like described above. If you are on NixOS, you also need to enable nix-ld.

Go to the zksync folder and run nix develop --impure. After it finishes, you are in a shell that has all the dependencies.

Environment
Edit the lines below and add them to your shell profile file (e.g. ~/.bash_profile, ~/.zshrc):

# Add path here:
export ZKSYNC_HOME=/path/to/zksync

export PATH=$ZKSYNC_HOME/bin:$PATH

# If you're like me, uncomment:
# cd $ZKSYNC_HOME
Tip: mold
Optionally, you may want to optimize the build time with the modern linker, mold.

This linker will speed up the build times, which can be pretty big for Rust binaries.

Follow the instructions in the repo in order to install it and enable for Rust.

Tip: Speeding up building RocksDB
By default, each time you compile rocksdb crate, it will compile required C++ sources from scratch. It can be avoided by using precompiled versions of library, and it will significantly improve your build times.

In order to do so, you can put compiled libraries to some persistent location, and add the following to your shell configuration file (e.g. .zshrc or .bashrc):

export ROCKSDB_LIB_DIR=<library location>
export SNAPPY_LIB_DIR=<library location>
Make sure that compiled libraries match the current version of RocksDB. One way to obtain them, is to compile the project in the usual way once, and then take built libraries from target/{debug,release}/build/librocksdb-sys-{some randomNode.jspackage.Makekingdom hearts 
Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.

Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.
Scope

This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.

This Code of Conduct also applies to actions taken outside of these spaces, and which have a negative impact on community health.
Enforcement and Reporting

We encourage all communities to resolve issues on their own whenever possible. Instances of abusive, harassing, or otherwise unacceptable behavior should be reported to the community leaders responsible for enforcement in a given project or to opencode@microsoft.com. If you are a Microsoft employee looking for support, please use the Community 911 reporting process.

Your report will be handled in accordance with the issue resolution process described in the Code of Conduct FAQ. All project and community leaders are obligated to respect the privacy and security of the reporter of any incident.
Attribution

This Code of Conduct is adapted from the Contributor Covenant, version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.

Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.

Expanding scope to include external impact on community health inspired by Facebook's Open Source Code of Conduct and Mozilla's Community Participation Guidelines.

For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.

    Twitter icon OpenAtMicrosoft
    GitHub icon microsoft/opensource.microsoft.com





















































































































































































































	
opencode@microsoft.comhttps://www.contributor-covenant.org/version/2/0/code_of_conduct.html.https://www.contributor-covenant.org/faq.911https://www.contributor-covenant.org/faq.https://www.contributor-covenant.org/translations., e.g., cat-2.png
### Image 3: <image>       % some image, e.g., cat-3.png
### Question: <question>   % What's the difference between the three cats?
### Answer: <answer>       % The colors of the three cats are different.
...
The training experience of DeepSpeed-VisualChat is straightforward and convenient. Here we give an example based on the CLIP visual encoder and the LLaMa-7B model:

git clone https://github.com/microsoft/DeepSpeedExamples.git
cd DeepSpeedExamples/applications/DeepSpeed-VisualChat/
pip install -r requirements.txt
cd training
bash training_scripts/run_7b.sh
The trained checkpoint will be automatically saved in a Hugging Face-compatible version and can be used to launch your own visual chat API:

cd ../chat
bash chat_scripts/run.sh # You need to change necessary variables, e.g, ckpt path
To support larger model inference, we have incorporated Hugging Face large model inference into our DeepSpeed-VisualChat API. Therefore, users can choose a different number of GPUs based on the GPU memory capacity and the model size.

Please refer to our GitHub Landing Page for more details.

7. Release: Try DeepSpeed-VisualChat today!
We are very excited to share that DeepSpeed-VisualChat is now open-sourced and available to the AI community.

To get started, please visit our GitHub page for DeepSpeed-VisualChat: GitHub Landing Page

We will continue to improve DeepSpeed-VisualChat with your feedback and support. Our roadmap shows currently supported features as well as ones that are planned for the future.

DeepSpeed-VisualChat is a component of the larger DeepSpeed ecosystem, which includes a range of Deep Learning systems and modeling technologies. To learn more,

Please visit our website for detailed blog posts, tutorials, and helpful documentation.
Follow us on our English X(Twitter), Japanese X(Twitter), and Chinese Zhihu for latest news on DeepSpeed.
We welcome your contributions to DeepSpeed! We encourage you to report issues, contribute PRs, and join discussions on the DeepSpeed GitHub page. Please see our contributing guide for more details. We are open to collaborations with universities, research labs, companies, such as those working together on deep learning research, applying DeepSpeed to empower real-world AI models and applications, and so on. For such requests (and other requests unsuitable for GitHub), please directly email to deepspeed-info@microsoft.com.

"Star" our DeepSpeed GitHub and DeepSpeedExamples GitHub cat-2.pngcat-3.pnghttps://github.com/microsoft/DeepSpeedExamples.gitrequirements.txt  title={{DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via Multi-Modal Causal Attention}},
  author={Zhewei Yao and Xiaoxia Wu and Conglong Li and Minjia Zhang and Heyang Qin and Olatunji Ruwase and Ammar Ahmad Awan and Samyam Rajbhandari and Yuxiong He},
  journal={arXiv preprint arXiv:2309.14327},
  year={2023}
}
1. Overview
Large Language models (LLMs), such as GPT and LLaMa, have showcased exceptional prowess in a myriad of text generation and comprehension tasks, especially when subjected to zero-/few-shot learning, particularly after instructed fine-tuning. However, to equip AI agents for diverse tasks, one critical feature that needs to be incorporated is multi-modal capability; for instance, the AI agent should be able to read images, hear voices, watch videos, etc. This capability is largely absent in solely text-based LLMs.

Recently, one of the research/practice mainstreams has begun exploring the incorporation of visual capability into LLMs, especially enabling LLMs to understand images by inserting raw pictures (referred to as large visual language models, or LVLMs in short).

The main caveats of the majority of existing works are:

The focus is predominantly on tasks related to a single image, such as visual question answering and captioning, or on handling multiple images that require concurrent input. Neither approach adeptly manages interleaved image-and-text input.
The scalability of the system is limited to models with ~10B parameters, which is about an order of magnitude smaller than largest open-sourced models.
However, for a genuine AI chat agent, the content of inputs could be multiple images interleaved with text, a situation rarely addressed by current works. Also, the generation capability of LLMs grows quickly as the model size increases. Therefore, focusing system capability on ~10B models limits further exploration of the potential of LVLMs.

To resolve these issues, we are introducing DeepSpeed-VisualChat (see arxiv report for more details) with the following new features:

Fully Open-Sourced Multi-round Multi-image Framework with Unprecedented Scalability: DeepSpeed-VisualChat, one of the pioneering fully open-sourced frameworks, enables multi-round and multi-image dialogues, accommodating interleaved text-and-image inputs. We leverage DeepSpeed to enhance our training with a 2B visual encoder and a 70B LLaMA-2 decoder model, illustrating the remarkable scalability of our framework.
Multi-Modal Causal Attention (MMCA) We devise a novel MMCA for multi-modal models that computes attention weights independently across various modalities. MMCA achieves objectives analogous to conventional cross-attention mechanisms but offers enhanced causal attention interpretations for generative tasks, eliminating the need for additional modules or parameters. It also presents superior training data efficiency compared to standard causal attention.
Data Blending for Interleaved Inputs To facilitate conversations with interleaved modalities, DeepSpeed-VisualChat employs assorted data blending techniques on existing datasets, overcoming the shortage of interleaved text-and-image inputs in most available open-source datasets.
2 Model architecture overview
deepspeed-info@microsoft.com2 dependencies
Run npm install and verify if everything still works correctly
Merge your feature branch into V3
Create a new branch from V3 and name it with the version tag
Run npm run pre-publish and set new custom version
Update version in ConstantsUtil in @web3modal/scaffold-utils to the correct version
Create a new PR with Release Notes and merge into V3
Checkout V3 and run npm run publish:latest
Draft a new release in GitHub and create new tag
Click on Generate Change and only leave the link with difference. Paste in your changelog from PR.
Check Set as the last release and publishnpx create-wc-dapp@latest -y w3mkingdom hearts  dependencies
Run npm install and verify if everything still works correctly
Merge your feature branch into V3
Create a new branch from V3 and name it with the version tag
Run npm run pre-publish and set new custom version
Update version in ConstantsUtil in @web3modal/scaffold-utils to the correct version
Create a new PR with Release Notes and merge into V3
Checkout V3 and run npm run publish:latest
Draft a new release in GitHub and create new tag
Click on Generate Change and only leave the link with difference. Paste in your changelog from PR.
Check Set as the last release and publishkingdom hearts 

-b, --branch <string>
    repository branch
--default-permissions
    do not prompt to accept additional permissions requested by the codespace
--devcontainer-path <string>
    path to the devcontainer.json file to use when creating codespace
-d, --display-name <string>
    display name for the codespace
--idle-timeout <duration>
    allowed inactivity before codespace is stopped, e.g. "10m", "1h"
-l, --location <string>
    location: {EastUs|SouthEastAsia|WestEurope|WestUs2} (determined automatically if not provided)
-m, --machine <string>
    hardware specifications for the VM
-R, --repo <string>
    repository name with owner: user/repo
--retention-period <duration>
    allowed time after shutting down before the codespace is automatically deleted (maximum 30 days), e.g. "1h", "72h"
-s, --status
    show status of post-create command and dotfiles
-w, --web
    create codespace from browser, cannot be used with --display-name, --idle-timeout, or --retention-period devcontainer.json30opencode@microsoft.comCommunity leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.

Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.
Scope

This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.

This Code of Conduct also applies to actions taken outside of these spaces, and which have a negative impact on community health.
Enforcement and Reporting

We encourage all communities to resolve issues on their own whenever possible. Instances of abusive, harassing, or otherwise unacceptable behavior should be reported to the community leaders responsible for enforcement in a given project or to opencode@microsoft.com. If you are a Microsoft employee looking for support, please use the Community 911 reporting process.

Your report will be handled in accordance with the issue resolution process described in the Code of Conduct FAQ. All project and community leaders are obligated to respect the privacy and security of the reporter of any incident.
Attribution

This Code of Conduct is adapted from the Contributor Covenant, version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.

Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.

Expanding scope to include external impact on community health inspired by Facebook's Open Source Code of Conduct and Mozilla's Community Participation Guidelines.

For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.

    Twitter icon OpenAtMicrosoft
    GitHub icon microsoft/opensource.microsoft.com





















































































































































































































	
911https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.https://www.contributor-covenant.org/faq., e.g., cat-2.png
### Image 3: <image>       % some image, e.g., cat-3.png
### Question: <question>   % What's the difference between the three cats?
### Answer: <answer>       % The colors of the three cats are different.
...
The training experience of DeepSpeed-VisualChat is straightforward and convenient. Here we give an example based on the CLIP visual encoder and the LLaMa-7B model:

git clone https://github.com/microsoft/DeepSpeedExamples.git
cd DeepSpeedExamples/applications/DeepSpeed-VisualChat/
pip install -r requirements.txt
cd training
bash training_scripts/run_7b.sh
The trained checkpoint will be automatically saved in a Hugging Face-compatible version and can be used to launch your own visual chat API:

cd ../chat
bash chat_scripts/run.sh # You need to change necessary variables, e.g, ckpt path
To support larger model inference, we have incorporated Hugging Face large model inference into our DeepSpeed-VisualChat API. Therefore, users can choose a different number of GPUs based on the GPU memory capacity and the model size.

Please refer to our GitHub Landing Page for more details.

7. Release: Try DeepSpeed-VisualChat today!
We are very excited to share that DeepSpeed-VisualChat is now open-sourced and available to the AI community.

To get started, please visit our GitHub page for DeepSpeed-VisualChat: GitHub Landing Page

We will continue to improve DeepSpeed-VisualChat with your feedback and support. Our roadmap shows currently supported features as well as ones that are planned for the future.

DeepSpeed-VisualChat is a component of the larger DeepSpeed ecosystem, which includes a range of Deep Learning systems and modeling technologies. To learn more,

Please visit our website for detailed blog posts, tutorials, and helpful documentation.
Follow us on our English X(Twitter), Japanese X(Twitter), and Chinese Zhihu for latest news on DeepSpeed.
We welcome your contributions to DeepSpeed! We encourage you to report issues, contribute PRs, and join discussions on the DeepSpeed GitHub page. Please see our contributing guide for more details. We are open to collaborations with universities, research labs, companies, such as those working together on deep learning research, applying DeepSpeed to empower real-world AI models and applications, and so on. For such requests (and other requests unsuitable for GitHub), please directly email to deepspeed-info@microsoft.com.

"Star" our DeepSpeed GitHub and DeepSpeedExamples GitHub https://www.contributor-covenant.org/translations.cat-2.pngcat-3.pnghttps://github.com/microsoft/DeepSpeedExamples.gitrequirements.txtdeepspeed-info@microsoft.com  title={{DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via Multi-Modal Causal Attention}},
  author={Zhewei Yao and Xiaoxia Wu and Conglong Li and Minjia Zhang and Heyang Qin and Olatunji Ruwase and Ammar Ahmad Awan and Samyam Rajbhandari and Yuxiong He},
  journal={arXiv preprint arXiv:2309.14327},
  year={2023}
}
1. Overview
Large Language models (LLMs), such as GPT and LLaMa, have showcased exceptional prowess in a myriad of text generation and comprehension tasks, especially when subjected to zero-/few-shot learning, particularly after instructed fine-tuning. However, to equip AI agents for diverse tasks, one critical feature that needs to be incorporated is multi-modal capability; for instance, the AI agent should be able to read images, hear voices, watch videos, etc. This capability is largely absent in solely text-based LLMs.

Recently, one of the research/practice mainstreams has begun exploring the incorporation of visual capability into LLMs, especially enabling LLMs to understand images by inserting raw pictures (referred to as large visual language models, or LVLMs in short).

The main caveats of the majority of existing works are:

The focus is predominantly on tasks related to a single image, such as visual question answering and captioning, or on handling multiple images that require concurrent input. Neither approach adeptly manages interleaved image-and-text input.
The scalability of the system is limited to models with ~10B parameters, which is about an order of magnitude smaller than largest open-sourced models.
However, for a genuine AI chat agent, the content of inputs could be multiple images interleaved with text, a situation rarely addressed by current works. Also, the generation capability of LLMs grows quickly as the model size increases. Therefore, focusing system capability on ~10B models limits further exploration of the potential of LVLMs.

To resolve these issues, we are introducing DeepSpeed-VisualChat (see arxiv report for more details) with the following new features:

Fully Open-Sourced Multi-round Multi-image Framework with Unprecedented Scalability: DeepSpeed-VisualChat, one of the pioneering fully open-sourced frameworks, enables multi-round and multi-image dialogues, accommodating interleaved text-and-image inputs. We leverage DeepSpeed to enhance our training with a 2B visual encoder and a 70B LLaMA-2 decoder model, illustrating the remarkable scalability of our framework.
Multi-Modal Causal Attention (MMCA) We devise a novel MMCA for multi-modal models that computes attention weights independently across various modalities. MMCA achieves objectives analogous to conventional cross-attention mechanisms but offers enhanced causal attention interpretations for generative tasks, eliminating the need for additional modules or parameters. It also presents superior training data efficiency compared to standard causal attention.
Data Blending for Interleaved Inputs To facilitate conversations with interleaved modalities, DeepSpeed-VisualChat employs assorted data blending techniques on existing datasets, overcoming the shortage of interleaved text-and-image inputs in most available open-source datasets.
2 Model architecture overview
 dependencies
Run npm install and verify if everything still works correctly
Merge your feature branch into V3
Create a new branch from V3 and name it with the version tag
Run npm run pre-publish and set new custom version
Update version in ConstantsUtil in @web3modal/scaffold-utils to the correct version
Create a new PR with Release Notes and merge into V3
Checkout V3 and run npm run publish:latest
Draft a new release in GitHub and create new tag
Click on Generate Change and only leave the link with difference. Paste in your changelog from PR.
Check Set as the last release and publishkingdom hearts 
Describe the bug**
A clear and concise description of what the bug is.

**To Reproduce**
Steps to reproduce the behavior:
1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

**Expected behavior**
A clear and concise description of what you expected to happen.

**Screenshots**
If applicable, add screenshots to help explain your problem.

**Desktop (please complete the following information):**
 - OS: [e.g. iOS]
 - Browser [e.g. chrome, safari]
 - Version [e.g. 22]

**Smartphone (please complete the following information):**
 - Device: [e.g. iPhone6]
 - OS: [e.g. iOS8.1]
 - Browser [e.g. stock browser, safari]
 - Version [e.g. 22]

**Additional context**
Add any other context about the problem here.
Start a new codespace

Click the Code button on your repository's landing page.
Click the Codespaces tab.
Click Create codespaces on main to create the codespace.
After the codespace has initialized there will be a terminal present.
Verify the GitHub Actions Importer CLI is installed and working. More information on the GitHub Actions Importer extension for the official GitHub CLI can be found here.

Run the following command in the codespace terminal:

gh actions-importer version
Verify the output is similar to below.

$ gh actions-importer version
gh version 2.14.3 (2022-07-26)
gh actions-importer        github/gh-actions-importer v0.1.12
actions-importer/cli       unknown
If gh actions-importer version did not produce similar output, please refer to the troubleshooting section.

Bootstrap a GitLab server
Execute the GitLab setup script that will start a container with GitLab running inside of it. The script should be executed when starting a new codespace or restarting an existing one.

Run the following command from the codespace terminal:

./gitlab/bootstrap/setup.sh
After some time, a pop-up box should appear with a link to the URL for your GitLab server.

You can also access the URL by going to the Ports tab in your terminal. Right-click the URL listed under the Local Address and click the Open in Browser tab.

Open the GitLab server in your browser and use the following credentials to authenticate:

Username: root
Password: actions-importer-labs!
Once authenticated, you should see a GitLab server with a few predefined pipelines in the actions-importer group.

Labs for GitLab
Perform the following labs to learn more about Actions migrations with GitHub Actions Importer:

Configure credentials for GitHub Actions Importer
Perform an audit on GitLab pipelines
Forecast potential build runner usage
Perform a dry-run migration of a GitLab pipeline
Use custom transformers to customize GitHub Actions Importer's behavior
Perform a production migration of a GitLab pipeline
Troubleshoot the GitHub Actions Importer CLI
The CLI extension for GitHub Actions Importer can be manually installed by following these steps:

Verify you are in the codespace terminal

Run this command from within the codespace terminal:

gh extension install github/gh-actions-importer
Verify the result of the install contains:

$ gh extension install github/gh-actions-importer
âœ“ Installed extension github/gh-actions-importer
Verify GitHub Actions Importer CLI extension is installed and working by running the following command from the codespace terminal:

gh actions-importer version
**To Reproduce**
Steps to reproduce the behavior:
1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

**Expected behavior**
A clear and concise description of what you expected to happen.

**Screenshots**
If applicable, add screenshots to help explain your problem.

**Desktop (please complete the following information):**
 - OS: [e.g. iOS]
 - Browser [e.g. chrome, safari]
 - Version [e.g. 22]

**Smartphone (please complete the following information):**
 - Device: [e.g. iPhone6]
 - OS: [e.g. iOS8.1]
 - Browser [e.g. stock browser, safari]
 - Version [e.g. 22]

**Additional context**
Add any other context about the problem here.**Is your feature request related to a problem? Please describe.**
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...]

**Describe the solution you'd like**
A clear and concise description of what you want to happen.

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.https://github.com/bradford80USA/unamed/actions/workflows/azure-container-webapp.yml1998350Program Overview
Privileges of Membership
Over 1300 organizations and individuals parti-
cipate in the JCP program. While there are no
obligatory duties, members have the opportunity
to influence the evolution of Java technology
through the development of Java Specification
Requests (JSR).
Members can license their Java specifications
under a variety of licenses, including open source
options. Anyone must be able to create an indepen-
dent implementation as long as they license and pass the
TCK to ensure compatibility. Members must also
make the option available to license the TCK and
RI separately. In addition, individuals, educational
organizations, and qualified nonprofits must have
access to the TCKs free of charge.
Successful Members:
â€¢ Review proposed JSRs and drafts
â€¢ Submit JSRs
â€¢ Nominate themselves or others to serve
on Expert Groups, which create or revise
specifications
â€¢ Build independent implementations
â€¢ Vote on EC membership ballots
â€¢ Nominate themselves for an EC seat
Members of an Expert Group may also:
â€¢ Serve as the Specification Lead of an
Expert Group
â€¢ Select others to join their Expert Group
â€¢ Use feedback from members and the public
to improve the quality of a specification
â€¢ Complete a specification, its RI, and its
associated TCK
â€¢ Maintain a specification after it is written
How to Become a Member
A person or organization can become a member
by signing the Java Specification Participation
Agreement (JSPA). This agreement between an
organization or individual and Oracle establishes
each memberâ€™s rights and obligations when partici-
pating in the JCP program. To cover costs, the JSPA
charges a nominal fee for commercial entities, but it
is free for Java User Groups and individuals.
The Java Specification Review Process
Currently, over 350 JSRs are in development.
A specification follows four major steps as it
progresses through the process, as shown in
the timeline.
1. INITIATION: A specification is initiated by one or
more members and approved for development
by the Executive Committee.
2. EARLY DRAFT: A group of experts is formed to
draft the specification for the public, community
and the Executive Committee to review. The
Expert Group uses feedback from the review to
revise the specification.
3. PUBLIC DRAFT: The draft is posted on the Internet
for a second review by the public. The Expert
Group uses the feedback to refine the document.
The Executive Committee decides if the draft
should proceed to the next step. The Specification
Lead ensures that the RI and its associated TCK
are completed before sending the specification to
the Executive Committee for final approval.
Java Community Process Program Overview
The Java Community Process (JCP) program is the formalization of the open, inclusive
process that has been used since 1998 to develop and revise Java technology specifications,
reference implementations (RI), and technology compatibility kits (TCK). Jav1300https://github.com/bradford80USA/unamed/actions/workflows/azure-container-webapp.yml1998350Program35013001998
-b, --branch <string>
    repository branch
--default-permissions
    do not prompt to accept additional permissions requested by the codespace
--devcontainer-path <string>
    path to the devcontainer.json file to use when creating codespace
-d, --display-name <string>
    display name for the codespace
--idle-timeout <duration>
    allowed inactivity before codespace is stopped, e.g. "10m", "1h"
-l, --location <string>
    location: {EastUs|SouthEastAsia|WestEurope|WestUs2} (determined automatically if not provided)
-m, --machine <string>
    hardware specifications for the VM
-R, --repo <string>
    repository name with owner: user/repo
--retention-period <duration>
    allowed time after shutting down before the codespace is automatically deleted (maximum 30 days), e.g. "1h", "72h"
-s, --status
    show status of post-create command and dotfiles
-w, --web
    create codespace from browser, cannot be used with --display-name, --idle-timeout, or --retention-period devcontainer.jsonCommunity leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.

Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.
Scope

This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.

This Code of Conduct also applies to actions taken outside of these spaces, and which have a negative impact on community health.
Enforcement and Reporting

We encourage all communities to resolve issues on their own whenever possible. Instances of abusive, harassing, or otherwise unacceptable behavior should be reported to the community leaders responsible for enforcement in a given project or to opencode@microsoft.com. If you are a Microsoft employee looking for support, please use the Community 911 reporting process.

Your report will be handled in accordance with the issue resolution process described in the Code of Conduct FAQ. All project and community leaders are obligated to respect the privacy and security of the reporter of any incident.
Attribution

This Code of Conduct is adapted from the Contributor Covenant, version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.

Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.

Expanding scope to include external impact on community health inspired by Facebook's Open Source Code of Conduct and Mozilla's Community Participation Guidelines.

For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.

    Twitter icon OpenAtMicrosoft
    GitHub icon microsoft/opensource.microsoft.com





















































































































































































































	
30opencode@microsoft.com911https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.https://www.contributor-covenant.org/faq., e.g., cat-2.png
### Image 3: <image>       % some image, e.g., cat-3.png
### Question: <question>   % What's the difference between the three cats?
### Answer: <answer>       % The colors of the three cats are different.
...
The training experience of DeepSpeed-VisualChat is straightforward and convenient. Here we give an example based on the CLIP visual encoder and the LLaMa-7B model:

git clone https://github.com/microsoft/DeepSpeedExamples.git
cd DeepSpeedExamples/applications/DeepSpeed-VisualChat/
pip install -r requirements.txt
cd training
bash training_scripts/run_7b.sh
The trained checkpoint will be automatically saved in a Hugging Face-compatible version and can be used to launch your own visual chat API:

cd ../chat
bash chat_scripts/run.sh # You need to change necessary variables, e.g, ckpt path
To support larger model inference, we have incorporated Hugging Face large model inference into our DeepSpeed-VisualChat API. Therefore, users can choose a different number of GPUs based on the GPU memory capacity and the model size.

Please refer to our GitHub Landing Page for more details.

7. Release: Try DeepSpeed-VisualChat today!
We are very excited to share that DeepSpeed-VisualChat is now open-sourced and available to the AI community.

To get started, please visit our GitHub page for DeepSpeed-VisualChat: GitHub Landing Page

We will continue to improve DeepSpeed-VisualChat with your feedback and support. Our roadmap shows currently supported features as well as ones that are planned for the future.

DeepSpeed-VisualChat is a component of the larger DeepSpeed ecosystem, which includes a range of Deep Learning systems and modeling technologies. To learn more,

Please visit our website for detailed blog posts, tutorials, and helpful documentation.
Follow us on our English X(Twitter), Japanese X(Twitter), and Chinese Zhihu for latest news on DeepSpeed.
We welcome your contributions to DeepSpeed! We encourage you to report issues, contribute PRs, and join discussions on the DeepSpeed GitHub page. Please see our contributing guide for more details. We are open to collaborations with universities, research labs, companies, such as those working together on deep learning research, applying DeepSpeed to empower real-world AI models and applications, and so on. For such requests (and other requests unsuitable for GitHub), please directly email to deepspeed-info@microsoft.com.

"Star" our DeepSpeed GitHub and DeepSpeedExamples GitHub https://www.contributor-covenant.org/translations.cat-2.pngcat-3.pngrequirements.txthttps://github.com/microsoft/DeepSpeedExamples.git  title={{DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via Multi-Modal Causal Attention}},
  author={Zhewei Yao and Xiaoxia Wu and Conglong Li and Minjia Zhang and Heyang Qin and Olatunji Ruwase and Ammar Ahmad Awan and Samyam Rajbhandari and Yuxiong He},
  journal={arXiv preprint arXiv:2309.14327},
  year={2023}
}
1. Overview
Large Language models (LLMs), such as GPT and LLaMa, have showcased exceptional prowess in a myriad of text generation and comprehension tasks, especially when subjected to zero-/few-shot learning, particularly after instructed fine-tuning. However, to equip AI agents for diverse tasks, one critical feature that needs to be incorporated is multi-modal capability; for instance, the AI agent should be able to read images, hear voices, watch videos, etc. This capability is largely absent in solely text-based LLMs.

Recently, one of the research/practice mainstreams has begun exploring the incorporation of visual capability into LLMs, especially enabling LLMs to understand images by inserting raw pictures (referred to as large visual language models, or LVLMs in short).

The main caveats of the majority of existing works are:

The focus is predominantly on tasks related to a single image, such as visual question answering and captioning, or on handling multiple images that require concurrent input. Neither approach adeptly manages interleaved image-and-text input.
The scalability of the system is limited to models with ~10B parameters, which is about an order of magnitude smaller than largest open-sourced models.
However, for a genuine AI chat agent, the content of inputs could be multiple images interleaved with text, a situation rarely addressed by current works. Also, the generation capability of LLMs grows quickly as the model size increases. Therefore, focusing system capability on ~10B models limits further exploration of the potential of LVLMs.

To resolve these issues, we are introducing DeepSpeed-VisualChat (see arxiv report for more details) with the following new features:

Fully Open-Sourced Multi-round Multi-image Framework with Unprecedented Scalability: DeepSpeed-VisualChat, one of the pioneering fully open-sourced frameworks, enables multi-round and multi-image dialogues, accommodating interleaved text-and-image inputs. We leverage DeepSpeed to enhance our training with a 2B visual encoder and a 70B LLaMA-2 decoder model, illustrating the remarkable scalability of our framework.
Multi-Modal Causal Attention (MMCA) We devise a novel MMCA for multi-modal models that computes attention weights independently across various modalities. MMCA achieves objectives analogous to conventional cross-attention mechanisms but offers enhanced causal attention interpretations for generative tasks, eliminating the need for additional modules or parameters. It also presents superior training data efficiency compared to standard causal attention.
Data Blending for Interleaved Inputs To facilitate conversations with interleaved modalities, DeepSpeed-VisualChat employs assorted data blending techniques on existing datasets, overcoming the shortage of interleaved text-and-image inputs in most available open-source datasets.
2 Model architecture overview
deepspeed-info@microsoft.com2kingdom
 hearts 
Describe the bug**
A clear and concise description of what the bug is.

**To Reproduce**
Steps to reproduce the behavior:
1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

**Expected behavior**
A clear and concise description of what you expected to happen.

**Screenshots**
If applicable, add screenshots to help explain your problem.

**Desktop (please complete the following information):**
 - OS: [e.g. iOS]
 - Browser [e.g. chrome, safari]
 - Version [e.g. 22]

**Smartphone (please complete the following information):**
 - Device: [e.g. iPhone6]
 - OS: [e.g. iOS8.1]
 - Browser [e.g. stock browser, safari]
 - Version [e.g. 22]

**Additional context**
Add any other context about the problem here.
Start a new codespace

Click the Code button on your repository's landing page.
Click the Codespaces tab.
Click Create codespaces on main to create the codespace.
After the codespace has initialized there will be a terminal present.
Verify the GitHub Actions Importer CLI is installed and working. More information on the GitHub Actions Importer extension for the official GitHub CLI can be found here.

Run the following command in the codespace terminal:

gh actions-importer version
Verify the output is similar to below.

$ gh actions-importer version
gh version 2.14.3 (2022-07-26)
gh actions-importer        github/gh-actions-importer v0.1.12
actions-importer/cli       unknown
If gh actions-importer version did not produce similar output, please refer to the troubleshooting section.

Bootstrap a GitLab server
Execute the GitLab setup script that will start a container with GitLab running inside of it. The script should be executed when starting a new codespace or restarting an existing one.

Run the following command from the codespace terminal:

./gitlab/bootstrap/setup.sh
After some time, a pop-up box should appear with a link to the URL for your GitLab server.

You can also access the URL by going to the Ports tab in your terminal. Right-click the URL listed under the Local Address and click the Open in Browser tab.

Open the GitLab server in your browser and use the following credentials to authenticate:

Username: root
Password: actions-importer-labs!
Once authenticated, you should see a GitLab server with a few predefined pipelines in the actions-importer group.

Labs for GitLab
Perform the following labs to learn more about Actions migrations with GitHub Actions Importer:

Configure credentials for GitHub Actions Importer
Perform an audit on GitLab pipelines
Forecast potential build runner usage
Perform a dry-run migration of a GitLab pipeline
Use custom transformers to customize GitHub Actions Importer's behavior
Perform a production migration of a GitLab pipeline
Troubleshoot the GitHub Actions Importer CLI
The CLI extension for GitHub Actions Importer can be manually installed by following these steps:

Verify you are in the codespace terminal

Run this command from within the codespace terminal:

gh extension install github/gh-actions-importer
Verify the result of the install contains:

$ gh extension install github/gh-actions-importer
âœ“ Installed extension github/gh-actions-importer
Verify GitHub Actions Importer CLI extension is installed and working by running the following command from the codespace terminal:

gh actions-importer version
**To Reproduce**
Steps to reproduce the behavior:
1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

**Expected behavior**
A clear and concise description of what you expected to happen.

**Screenshots**
If applicable, add screenshots to help explain your problem.

**Desktop (please complete the following information):**
 - OS: [e.g. iOS]
 - Browser [e.g. chrome, safari]
 - Version [e.g. 22]

**Smartphone (please complete the following information):**
 - Device: [e.g. iPhone6]
 - OS: [e.g. iOS8.1]
 - Browser [e.g. stock browser, safari]
 - Version [e.g. 22]

**Additional context**
Add any other context about the problem here.**Is your feature request related to a problem? Please describe.**
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...]

**Describe the solution you'd like**
A clear and concise description of what you want to happen.

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.https://github.com/bradford80USA/unamed/actions/workflows/azure-container-webapp.yml1998350Program Overview
Privileges of Membership
Over 1300 organizations and individuals parti-
cipate in the JCP program. While there are no
obligatory duties, members have the opportunity
to influence the evolution of Java technology
through the development of Java Specification
Requests (JSR).
Members can license their Java specifications
under a variety of licenses, including open source
options. Anyone must be able to create an indepen-
dent implementation as long as they license and pass the
TCK to ensure compatibility. Members must also
make the option available to license the TCK and
RI separately. In addition, individuals, educational
organizations, and qualified nonprofits must have
access to the TCKs free of charge.
Successful Members:
â€¢ Review proposed JSRs and drafts
â€¢ Submit JSRs
â€¢ Nominate themselves or others to serve
on Expert Groups, which create or revise
specifications
â€¢ Build independent implementations
â€¢ Vote on EC membership ballots
â€¢ Nominate themselves for an EC seat
Members of an Expert Group may also:
â€¢ Serve as the Specification Lead of an
Expert Group
â€¢ Select others to join their Expert Group
â€¢ Use feedback from members and the public
to improve the quality of a specification
â€¢ Complete a specification, its RI, and its
associated TCK
â€¢ Maintain a specification after it is written
How to Become a Member
A person or organization can become a member
by signing the Java Specification Participation
Agreement (JSPA). This agreement between an
organization or individual and Oracle establishes
each memberâ€™s rights and obligations when partici-
pating in the JCP program. To cover costs, the JSPA
charges a nominal fee for commercial entities, but it
is free for Java User Groups and individuals.
The Java Specification Review Process
Currently, over 350 JSRs are in development.
A specification follows four major steps as it
progresses through the process, as shown in
the timeline.
1. INITIATION: A specification is initiated by one or
more members and approved for development
by the Executive Committee.
2. EARLY DRAFT: A group of experts is formed to
draft the specification for the public, community
and the Executive Committee to review. The
Expert Group uses feedback from the review to
revise the specification.
3. PUBLIC DRAFT: The draft is posted on the Internet
for a second review by the public. The Expert
Group uses the feedback to refine the document.
The Executive Committee decides if the draft
should proceed to the next step. The Specification
Lead ensures that the RI and its associated TCK
are completed before sending the specification to
the Executive Committee for final approval.
Java Community Process Program Overview
The Java Community Process (JCP) program is the formalization of the open, inclusive
process that has been used since 1998 to develop and revise Java technology specifications,
reference implementations (RI), and technology compatibility kits (TCK). Jav1300https://github.com/bradford80USA/unamed/actions/workflows/azure-container-webapp.yml1998350Program13003501998
-b, --branch <string>
    repository branch
--default-permissions
    do not prompt to accept additional permissions requested by the codespace
--devcontainer-path <string>
    path to the devcontainer.json file to use when creating codespace
-d, --display-name <string>
    display name for the codespace
--idle-timeout <duration>
    allowed inactivity before codespace is stopped, e.g. "10m", "1h"
-l, --location <string>
    location: {EastUs|SouthEastAsia|WestEurope|WestUs2} (determined automatically if not provided)
-m, --machine <string>
    hardware specifications for the VM
-R, --repo <string>
    repository name with owner: user/repo
--retention-period <duration>
    allowed time after shutting down before the codespace is automatically deleted (maximum 30 days), e.g. "1h", "72h"
-s, --status
    show status of post-create command and dotfiles
-w, --web
    create codespace from browser, cannot be used with --display-name, --idle-timeout, or --retention-period 30Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.

Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.
Scope

This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.

This Code of Conduct also applies to actions taken outside of these spaces, and which have a negative impact on community health.
Enforcement and Reporting

We encourage all communities to resolve issues on their own whenever possible. Instances of abusive, harassing, or otherwise unacceptable behavior should be reported to the community leaders responsible for enforcement in a given project or to opencode@microsoft.com. If you are a Microsoft employee looking for support, please use the Community 911 reporting process.

Your report will be handled in accordance with the issue resolution process described in the Code of Conduct FAQ. All project and community leaders are obligated to respect the privacy and security of the reporter of any incident.
Attribution

This Code of Conduct is adapted from the Contributor Covenant, version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.

Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.

Expanding scope to include external impact on community health inspired by Facebook's Open Source Code of Conduct and Mozilla's Community Participation Guidelines.

For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.

    Twitter icon OpenAtMicrosoft
    GitHub icon microsoft/opensource.microsoft.com





















































































































































































































	
opencode@microsoft.com911https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.https://www.contributor-covenant.org/faq.https://www.contributor-covenant.org/translations., e.g., cat-2.png
### Image 3: <image>       % some image, e.g., cat-3.png
### Question: <question>   % What's the difference between the three cats?
### Answer: <answer>       % The colors of the three cats are different.
...
The training experience of DeepSpeed-VisualChat is straightforward and convenient. Here we give an example based on the CLIP visual encoder and the LLaMa-7B model:

git clone https://github.com/microsoft/DeepSpeedExamples.git
cd DeepSpeedExamples/applications/DeepSpeed-VisualChat/
pip install -r requirements.txt
cd training
bash training_scripts/run_7b.sh
The trained checkpoint will be automatically saved in a Hugging Face-compatible version and can be used to launch your own visual chat API:

cd ../chat
bash chat_scripts/run.sh # You need to change necessary variables, e.g, ckpt path
To support larger model inference, we have incorporated Hugging Face large model inference into our DeepSpeed-VisualChat API. Therefore, users can choose a different number of GPUs based on the GPU memory capacity and the model size.

Please refer to our GitHub Landing Page for more details.

7. Release: Try DeepSpeed-VisualChat today!
We are very excited to share that DeepSpeed-VisualChat is now open-sourced and available to the AI community.

To get started, please visit our GitHub page for DeepSpeed-VisualChat: GitHub Landing Page

We will continue to improve DeepSpeed-VisualChat with your feedback and support. Our roadmap shows currently supported features as well as ones that are planned for the future.

DeepSpeed-VisualChat is a component of the larger DeepSpeed ecosystem, which includes a range of Deep Learning systems and modeling technologies. To learn more,

Please visit our website for detailed blog posts, tutorials, and helpful documentation.
Follow us on our English X(Twitter), Japanese X(Twitter), and Chinese Zhihu for latest news on DeepSpeed.
We welcome your contributions to DeepSpeed! We encourage you to report issues, contribute PRs, and join discussions on the DeepSpeed GitHub page. Please see our contributing guide for more details. We are open to collaborations with universities, research labs, companies, such as those working together on deep learning research, applying DeepSpeed to empower real-world AI models and applications, and so on. For such requests (and other requests unsuitable for GitHub), please directly email to deepspeed-info@microsoft.com.

"Star" our DeepSpeed GitHub and DeepSpeedExamples GitHub cat-2.pngcat-3.pnghttps://github.com/microsoft/DeepSpeedExamples.gitrequirements.txt  title={{DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via Multi-Modal Causal Attention}},
  author={Zhewei Yao and Xiaoxia Wu and Conglong Li and Minjia Zhang and Heyang Qin and Olatunji Ruwase and Ammar Ahmad Awan and Samyam Rajbhandari and Yuxiong He},
  journal={arXiv preprint arXiv:2309.14327},
  year={2023}
}
1. Overview
Large Language models (LLMs), such as GPT and LLaMa, have showcased exceptional prowess in a myriad of text generation and comprehension tasks, especially when subjected to zero-/few-shot learning, particularly after instructed fine-tuning. However, to equip AI agents for diverse tasks, one critical feature that needs to be incorporated is multi-modal capability; for instance, the AI agent should be able to read images, hear voices, watch videos, etc. This capability is largely absent in solely text-based LLMs.

Recently, one of the research/practice mainstreams has begun exploring the incorporation of visual capability into LLMs, especially enabling LLMs to understand images by inserting raw pictures (referred to as large visual language models, or LVLMs in short).

The main caveats of the majority of existing works are:

The focus is predominantly on tasks related to a single image, such as visual question answering and captioning, or on handling multiple images that require concurrent input. Neither approach adeptly manages interleaved image-and-text input.
The scalability of the system is limited to models with ~10B parameters, which is about an order of magnitude smaller than largest open-sourced models.
However, for a genuine AI chat agent, the content of inputs could be multiple images interleaved with text, a situation rarely addressed by current works. Also, the generation capability of LLMs grows quickly as the model size increases. Therefore, focusing system capability on ~10B models limits further exploration of the potential of LVLMs.

To resolve these issues, we are introducing DeepSpeed-VisualChat (see arxiv report for more details) with the following new features:

Fully Open-Sourced Multi-round Multi-image Framework with Unprecedented Scalability: DeepSpeed-VisualChat, one of the pioneering fully open-sourced frameworks, enables multi-round and multi-image dialogues, accommodating interleaved text-and-image inputs. We leverage DeepSpeed to enhance our training with a 2B visual encoder and a 70B LLaMA-2 decoder model, illustrating the remarkable scalability of our framework.
Multi-Modal Causal Attention (MMCA) We devise a novel MMCA for multi-modal models that computes attention weights independently across various modalities. MMCA achieves objectives analogous to conventional cross-attention mechanisms but offers enhanced causal attention interpretations for generative tasks, eliminating the need for additional modules or parameters. It also presents superior training data efficiency compared to standard causal attention.
Data Blending for Interleaved Inputs To facilitate conversations with interleaved modalities, DeepSpeed-VisualChat employs assorted data blending techniques on existing datasets, overcoming the shortage of interleaved text-and-image inputs in most available open-source datasets.
2 Model architecture overview
deepspeed-info@microsoft.com2kingdomDescribe the bug**
A clear and concise description of what the bug is.

**To Reproduce**
Steps to reproduce the behavior:
1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

**Expected behavior**
A clear and concise description of what you expected to happen.

**Screenshots**
If applicable, add screenshots to help explain your problem.

**Desktop (please complete the following information):**
 - OS: [e.g. iOS]
 - Browser [e.g. chrome, safari]
 - Version [e.g. 22]

**Smartphone (please complete the following information):**
 - Device: [e.g. iPhone6]
 - OS: [e.g. iOS8.1]
 - Browser [e.g. stock browser, safari]
 - Version [e.g. 22]

**Additional context**
Add any other context about the problem here.
Start a new codespace

Click the Code button on your repository's landing page.
Click the Codespaces tab.
Click Create codespaces on main to create the codespace.
After the codespace has initialized there will be a terminal present.
Verify the GitHub Actions Importer CLI is installed and working. More information on the GitHub Actions Importer extension for the official GitHub CLI can be found here.

Run the following command in the codespace terminal:

gh actions-importer version
Verify the output is similar to below.

$ gh actions-importer version
gh version 2.14.3 (2022-07-26)
gh actions-importer        github/gh-actions-importer v0.1.12
actions-importer/cli       unknown
If gh actions-importer version did not produce similar output, please refer to the troubleshooting section.

Bootstrap a GitLab server
Execute the GitLab setup script that will start a container with GitLab running inside of it. The script should be executed when starting a new codespace or restarting an existing one.

Run the following command from the codespace terminal:

./gitlab/bootstrap/setup.sh
After some time, a pop-up box should appear with a link to the URL for your GitLab server.

You can also access the URL by going to the Ports tab in your terminal. Right-click the URL listed under the Local Address and click the Open in Browser tab.

Open the GitLab server in your browser and use the following credentials to authenticate:

Username: root
Password: actions-importer-labs!
Once authenticated, you should see a GitLab server with a few predefined pipelines in the actions-importer group.

Labs for GitLab
Perform the following labs to learn more about Actions migrations with GitHub Actions Importer:

Configure credentials for GitHub Actions Importer
Perform an audit on GitLab pipelines
Forecast potential build runner usage
Perform a dry-run migration of a GitLab pipeline
Use custom transformers to customize GitHub Actions Importer's behavior
Perform a production migration of a GitLab pipeline
Troubleshoot the GitHub Actions Importer CLI
The CLI extension for GitHub Actions Importer can be manually installed by following these steps:

Verify you are in the codespace terminal

Run this command from within the codespace terminal:

gh extension install github/gh-actions-importer
Verify the result of the install contains:

$ gh extension install github/gh-actions-importer
âœ“ Installed extension github/gh-actions-importer
Verify GitHub Actions Importer CLI extension is installed and working by running the following command from the codespace terminal:

gh actions-importer version
**To Reproduce**
Steps to reproduce the behavior:
1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

**Expected behavior**
A clear and concise description of what you expected to happen.

**Screenshots**
If applicable, add screenshots to help explain your problem.

**Desktop (please complete the following information):**
 - OS: [e.g. iOS]
 - Browser [e.g. chrome, safari]
 - Version [e.g. 22]

**Smartphone (please complete the following information):**
 - Device: [e.g. iPhone6]
 - OS: [e.g. iOS8.1]
 - Browser [e.g. stock browser, safari]
 - Version [e.g. 22]

**Additional context**
Add any other context about the problem here.**Is your feature request related to a problem? Please describe.**
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...]

**Describe the solution you'd like**
A clear and concise description of what you want to happen.

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.https://github.com/bradford80USA/unamed/actions/workflows/azure-container-webapp.yml1998350Program Overview
Privileges of Membership
Over 1300 organizations and individuals parti-
cipate in the JCP program. While there are no
obligatory duties, members have the opportunity
to influence the evolution of Java technology
through the development of Java Specification
Requests (JSR).
Members can license their Java specifications
under a variety of licenses, including open source
options. Anyone must be able to create an indepen-
dent implementation as long as they license and pass the
TCK to ensure compatibility. Members must also
make the option available to license the TCK and
RI separately. In addition, individuals, educational
organizations, and qualified nonprofits must have
access to the TCKs free of charge.
Successful Members:
â€¢ Review proposed JSRs and drafts
â€¢ Submit JSRs
â€¢ Nominate themselves or others to serve
on Expert Groups, which create or revise
specifications
â€¢ Build independent implementations
â€¢ Vote on EC membership ballots
â€¢ Nominate themselves for an EC seat
Members of an Expert Group may also:
â€¢ Serve as the Specification Lead of an
Expert Group
â€¢ Select others to join their Expert Group
â€¢ Use feedback from members and the public
to improve the quality of a specification
â€¢ Complete a specification, its RI, and its
associated TCK
â€¢ Maintain a specification after it is written
How to Become a Member
A person or organization can become a member
by signing the Java Specification Participation
Agreement (JSPA). This agreement between an
organization or individual and Oracle establishes
each memberâ€™s rights and obligations when partici-
pating in the JCP program. To cover costs, the JSPA
charges a nominal fee for commercial entities, but it
is free for Java User Groups and individuals.
The Java Specification Review Process
Currently, over 350 JSRs are in development.
A specification follows four major steps as it
progresses through the process, as shown in
the timeline.
1. INITIATION: A specification is initiated by one or
more members and approved for development
by the Executive Committee.
2. EARLY DRAFT: A group of experts is formed to
draft the specification for the public, community
and the Executive Committee to review. The
Expert Group uses feedback from the review to
revise the specification.
3. PUBLIC DRAFT: The draft is posted on the Internet
for a second review by the public. The Expert
Group uses the feedback to refine the document.
The Executive Committee decides if the draft
should proceed to the next step. The Specification
Lead ensures that the RI and its associated TCK
are completed before sending the specification to
the Executive Committee for final approval.
Java Community Process Program Overview
The Java Community Process (JCP) program is the formalization of the open, inclusive
process that has been used since 1998 to develop and revise Java technology specifications,
reference implementations (RI), and technology compatibility kits (TCK). Jav1300https://github.com/bradford80USA/unamed/actions/workflows/azure-container-webapp.yml1998350Program13003501998
-b, --branch <string>
    repository branch
--default-permissions
    do not prompt to accept additional permissions requested by the codespace
--devcontainer-path <string>
    path to the devcontainer.json file to use when creating codespace
-d, --display-name <string>
    display name for the codespace
--idle-timeout <duration>
    allowed inactivity before codespace is stopped, e.g. "10m", "1h"
-l, --location <string>
    location: {EastUs|SouthEastAsia|WestEurope|WestUs2} (determined automatically if not provided)
-m, --machine <string>
    hardware specifications for the VM
-R, --repo <string>
    repository name with owner: user/repo
--retention-period <duration>
    allowed time after shutting down before the codespace is automatically deleted (maximum 30 days), e.g. "1h", "72h"
-s, --status
    show status of post-create command and dotfiles
-w, --web
    create codespace from browser, cannot be used with --display-name, --idle-timeout, or --retention-period  hearts devcontainer.jsonCommunity leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.

Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.
Scope

This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.

This Code of Conduct also applies to actions taken outside of these spaces, and which have a negative impact on community health.
Enforcement and Reporting

We encourage all communities to resolve issues on their own whenever possible. Instances of abusive, harassing, or otherwise unacceptable behavior should be reported to the community leaders responsible for enforcement in a given project or to opencode@microsoft.com. If you are a Microsoft employee looking for support, please use the Community 911 reporting process.

Your report will be handled in accordance with the issue resolution process described in the Code of Conduct FAQ. All project and community leaders are obligated to respect the privacy and security of the reporter of any incident.
Attribution

This Code of Conduct is adapted from the Contributor Covenant, version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.

Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.

Expanding scope to include external impact on community health inspired by Facebook's Open Source Code of Conduct and Mozilla's Community Participation Guidelines.

For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.

    Twitter icon OpenAtMicrosoft
    GitHub icon microsoft/opensource.microsoft.com
, e.g., cat-2.png
### Image 3: <image>       % some image, e.g., cat-3.png
### Question: <question>   % What's the difference between the three cats?
### Answer: <answer>       % The colors of the three cats are different.
...
The training experience of DeepSpeed-VisualChat is straightforward and convenient. Here we give an example based on the CLIP visual encoder and the LLaMa-7B model:

git clone https://github.com/microsoft/DeepSpeedExamples.git
cd DeepSpeedExamples/applications/DeepSpeed-VisualChat/
pip install -r requirements.txt
cd training
bash training_scripts/run_7b.sh
The trained checkpoint will be automatically saved in a Hugging Face-compatible version and can be used to launch your own visual chat API:

cd ../chat
bash chat_scripts/run.sh # You need to change necessary variables, e.g, ckpt path
To support larger model inference, we have incorporated Hugging Face large model inference into our DeepSpeed-VisualChat API. Therefore, users can choose a different number of GPUs based on the GPU memory capacity and the model size.

Please refer to our GitHub Landing Page for more details.

7. Release: Try DeepSpeed-VisualChat today!
We are very excited to share that DeepSpeed-VisualChat is now open-sourced and available to the AI community.

To get started, please visit our GitHub page for DeepSpeed-VisualChat: GitHub Landing Page

We will continue to improve DeepSpeed-VisualChat with your feedback and support. Our roadmap shows currently supported features as well as ones that are planned for the future.

DeepSpeed-VisualChat is a component of the larger DeepSpeed ecosystem, which includes a range of Deep Learning systems and modeling technologies. To learn more,

Please visit our website for detailed blog posts, tutorials, and helpful documentation.
Follow us on our English X(Twitter), Japanese X(Twitter), and Chinese Zhihu for latest news on DeepSpeed.
We welcome your contributions to DeepSpeed! We encourage you to report issues, contribute PRs, and join discussions on the DeepSpeed GitHub page. Please see our contributing guide for more details. We are open to collaborations with universities, research labs, companies, such as those working together on deep learning research, applying DeepSpeed to empower real-world AI models and applications, and so on. For such requests (and other requests unsuitable for GitHub), please directly email to deepspeed-info@microsoft.com.

"Star" our DeepSpeed GitHub and DeepSpeedExamples GitHub https://www.contributor-covenant.org/translations.cat-2.pngcat-3.pngrequirements.txthttps://github.com/microsoft/DeepSpeedExamples.git  title={{DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via Multi-Modal Causal Attention}},
  author={Zhewei Yao and Xiaoxia Wu and Conglong Li and Minjia Zhang and Heyang Qin and Olatunji Ruwase and Ammar Ahmad Awan and Samyam Rajbhandari and Yuxiong He},
  journal={arXiv preprint arXiv:2309.14327},
  year={2023}
}
1. Overview
Large Language models (LLMs), such as GPT and LLaMa, have showcased exceptional prowess in a myriad of text generation and comprehension tasks, especially when subjected to zero-/few-shot learning, particularly after instructed fine-tuning. However, to equip AI agents for diverse tasks, one critical feature that needs to be incorporated is multi-modal capability; for instance, the AI agent should be able to read images, hear voices, watch videos, etc. This capability is largely absent in solely text-based LLMs.

Recently, one of the research/practice mainstreams has begun exploring the incorporation of visual capability into LLMs, especially enabling LLMs to understand images by inserting raw pictures (referred to as large visual language models, or LVLMs in short).

The main caveats of the majority of existing works are:

The focus is predominantly on tasks related to a single image, such as visual question answering and captioning, or on handling multiple images that require concurrent input. Neither approach adeptly manages interleaved image-and-text input.
The scalability of the system is limited to models with ~10B parameters, which is about an order of magnitude smaller than largest open-sourced models.
However, for a genuine AI chat agent, the content of inputs could be multiple images interleaved with text, a situation rarely addressed by current works. Also, the generation capability of LLMs grows quickly as the model size increases. Therefore, focusing system capability on ~10B models limits further exploration of the potential of LVLMs.

To resolve these issues, we are introducing DeepSpeed-VisualChat (see arxiv report for more details) with the following new features:

Fully Open-Sourced Multi-round Multi-image Framework with Unprecedented Scalability: DeepSpeed-VisualChat, one of the pioneering fully open-sourced frameworks, enables multi-round and multi-image dialogues, accommodating interleaved text-and-image inputs. We leverage DeepSpeed to enhance our training with a 2B visual encoder and a 70B LLaMA-2 decoder model, illustrating the remarkable scalability of our framework.
Multi-Modal Causal Attention (MMCA) We devise a novel MMCA for multi-modal models that computes attention weights independently across various modalities. MMCA achieves objectives analogous to conventional cross-attention mechanisms but offers enhanced causal attention interpretations for generative tasks, eliminating the need for additional modules or parameters. It also presents superior training data efficiency compared to standard causal attention.
Data Blending for Interleaved Inputs To facilitate conversations with interleaved modalities, DeepSpeed-VisualChat employs assorted data blending techniques on existing datasets, overcoming the shortage of interleaved text-and-image inputs in most available open-source datasets.
2 Model architecture overview
2kingdom hearts 




















































































































































































































	
